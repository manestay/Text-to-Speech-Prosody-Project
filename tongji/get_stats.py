"""
This script processes log files generated by training AuToBI models, and outputs statistics
for each model.

@author: Bryan Li (bl2557@columbia.edu)
"""

import argparse
from collections import defaultdict
import glob
from types import SimpleNamespace

TASKS = set(['phrase_accent_classification', 'pitch_accent_detection', 'intermediate_phrase_boundary_detection',
             'intonational_phrase_boundary_detection', 'pitch_accent_classification',
             'phrase_accent_boundary_tone_classification'])

parser = argparse.ArgumentParser(description='Process AuToBI log files.')
parser.add_argument('--model', required=True, help='model to process: dur, burnc, dur_old, bdc_burnc, tongji')
parser.add_argument('--affix', default='', help='affix to TextGrids folder')

Task = SimpleNamespace

def main():
    logs = []
    args = parser.parse_args()
    model_name = args.model
    affix = '_' + args.affix if args.affix else ''
    log_names = glob.glob('{}/TextGrids{}/*.log'.format(model_name, affix))
    print('processing {} logs'.format(len(log_names)))
    for log_name in sorted(log_names):
        with open(log_name, 'r') as f:
            lines = f.readlines()
        log = [log_name]
        i = 0
        j = 0
        task_index = []
        while i < len(lines):
            line = lines[i]
            words = line.split()
            if words and words[-1] in TASKS: # find index of each task
                task_index.append((words[-1], i))
            i += 1
        for task, index in task_index: # get values for each task
            task = Task()
            task.name = lines[index].split()[-1]
            task.mean = float(lines[index+1].split()[-1])
            task.stdev = float(lines[index+2].split()[-1])
            task.sterr = float(lines[index+3].split()[-1])
            task.conf = float(lines[index+4].split()[-1])
            task.N = int(lines[index+5].split()[-1])
            j = 11
            while True:
                if lines[index+j].strip():
                    words2 = lines[index+j].split()
                    if words2[0] == 'Mutual':
                        break
                j += 1
            task.mut_info = float(words2[-1])
            task.avg_recall = float(lines[index+j+1].split()[-1])
            task.ent_recall = float(lines[index+j+2].split()[-1])
            log.append(task)
        logs.append(log)
    dd = defaultdict(lambda: defaultdict(list))
    # dd['names'] = []
    for i, log in enumerate(logs):
        # dd['names'].append(log[0])
        print(i, log[0])
        for task in log[1:]: # accumulate values for each task
            task_name = task.name
            fields = task.__dict__
            # del fields['name']
            dd[task_name]['logname'].append(log[0])
            for k, v in fields.items():
                if isinstance(v, str):
                    continue
                dd[task_name][k].append(v)
    task_avgs = defaultdict(dict)
    import pdb; pdb.set_trace()
    for task_name, values in sorted(dd.items()):
        print(task_name)
        for k, v in sorted(values.items()):
            avg = sum(v) / len(v)
            print('avg {}: {}'.format(k, avg))
            task_avgs[task_name][k] = avg
        print()
    return task_avgs


if __name__ == "__main__":
    avgs = main()
