{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "from time import time\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "best_model = None\n",
    "\n",
    "dict_vectorizer = DictVectorizer()\n",
    "select_percentile = SelectPercentile(percentile=100)\n",
    "clf_map = [\n",
    "    (\n",
    "        MultinomialNB(),\n",
    "        {\n",
    "            'clf__alpha': [.001, .01, .1, 1],\n",
    "            'clf__fit_prior': [True, False],\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        LinearSVC(),\n",
    "        {\n",
    "            'clf__C': [1, 10, 100],\n",
    "            'clf__penalty': ['l2'],\n",
    "            'clf__loss': ['hinge', 'squared_hinge'],\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        LogisticRegression(),\n",
    "        {\n",
    "            'clf__penalty': ['l1','l2'],\n",
    "            'clf__fit_intercept': [True, False],\n",
    "            'clf__C':[1, 10, 100],\n",
    "        }\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "def classify_my_model(clf, param_grid, model_name):\n",
    "    global best_score, best_model\n",
    "    print('###################################',type(clf),'#########################################')\n",
    "    folds = StratifiedKFold(y_data, n_folds=3, shuffle=True, random_state=int(time()))\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "        ('selector', select_percentile),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "    param_grid['selector__percentile'] = [90, 95, 100]\n",
    "\n",
    "    gs = GridSearchCV(pipeline,\n",
    "                      param_grid,\n",
    "                      scoring='f1',\n",
    "                      cv=folds,\n",
    "                      n_jobs=-1,\n",
    "                      verbose=1)\n",
    "    t0 = time()\n",
    "    gs.fit(x_data, y_data)\n",
    "    train_time = time() - t0\n",
    "    print(\"Train time: %0.3fs\" % train_time)\n",
    "#     print(\"Real train time: %0.3fs\" % (train_time * (TOTAL_COUNT/DEV_COUNT)))\n",
    "    print(\"Best score: %0.3f\" % gs.best_score_)\n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_params[param_name]))\n",
    "    best_score = gs.best_score_\n",
    "    best_model = gs.best_estimator_\n",
    "    with open(model_name+'_best_'+type(clf).__name__+\".pkl\", 'wb') as handle:\n",
    "        pickle.dump(best_model, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best one was LogReg with only num of syllables as a continuous feature\n",
    "features = [\n",
    "    'word',\n",
    "    'word_pos_tag',\n",
    "    'word_pos_tag_simplified',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_turn',\n",
    "    'word_number_in_task',\n",
    "    'total_number_of_words_in_turn',\n",
    "    'total_number_of_words_in_task'\n",
    "]\n",
    "feat_indices = [18, 19, 20, 22, 11, 12, 14, 15]\n",
    "label_index = 27\n",
    "continuous_feats = [\n",
    "    'word_number_of_syllables'\n",
    "]\n",
    "## Read the file\n",
    "file_name = \"big-table-PoS.csv\"\n",
    "x_data = []\n",
    "y_data = []\n",
    "labels = []\n",
    "with open(file_name, 'r') as f:\n",
    "    for i, l in enumerate(csv.reader(f)):\n",
    "        if i == 0: continue\n",
    "#         elif i == 2: print x_data, y_data\n",
    "        feats = {feat: l[i] for feat, i in zip(features,feat_indices)}\n",
    "        # convert some to continuous features\n",
    "        for feat in continuous_feats:\n",
    "            feats[feat] = float(feats[feat])\n",
    "        x_data.append(feats)\n",
    "        label = l[label_index] == \"4\" or l[label_index] == \"4-\" or l[label_index] == \"4p\"\n",
    "        y_data.append(label)\n",
    "        labels.append(l[label_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance from end of ipu and turn\n",
    "DIST_END_TURN = \"DIST_END_TURN\"\n",
    "for i in range(len(x_data)):\n",
    "    x_data[i][DIST_END_TURN] = int(x_data[i]['total_number_of_words_in_turn']) - int(x_data[i]['word_number_in_turn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using turn pos bigrams\n",
    "POS_TURN_BIGRAM_LEFT = \"POS_TURN_BIGRAM_LEFT\"\n",
    "POS_TURN_BIGRAM_RIGHT = \"POS_TURN_BIGRAM_RIGHT\"\n",
    "for i in range(len(x_data)):\n",
    "    # to the left\n",
    "    if x_data[i]['word_number_in_turn'] == '1':\n",
    "        x_data[i][POS_TURN_BIGRAM_LEFT] = 'BEGIN/'+x_data[i]['word_pos_tag']\n",
    "    else:\n",
    "        x_data[i][POS_TURN_BIGRAM_LEFT] = x_data[i-1]['word_pos_tag']+\"/\"+x_data[i]['word_pos_tag']\n",
    "    # to the right\n",
    "    if x_data[i]['word_number_in_turn'] == x_data[i]['total_number_of_words_in_turn']:\n",
    "        x_data[i][POS_TURN_BIGRAM_RIGHT] = x_data[i]['word_pos_tag']+\"/END\"\n",
    "    else:\n",
    "        x_data[i][POS_TURN_BIGRAM_RIGHT] = x_data[i]['word_pos_tag']+\"/\"+x_data[i+1]['word_pos_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Trigram\n",
    "POS_IPU_TRIGRAM = \"POS_TURN_TRIGRAM\"\n",
    "for i in range(len(x_data)):\n",
    "    # to the left\n",
    "    left = \"BEGIN\"\n",
    "    if x_data[i]['word_number_in_turn'] != '1':\n",
    "        left = x_data[i-1]['word_pos_tag']\n",
    "    # to the right\n",
    "    right = \"END\"\n",
    "    if x_data[i]['word_number_in_turn'] != x_data[i]['total_number_of_words_in_turn']:\n",
    "        right = x_data[i+1]['word_pos_tag']\n",
    "    x_data[i][POS_IPU_TRIGRAM] = left+\"/\"+x_data[i]['word_pos_tag']+\"/\"+right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_STUTTER = \"IS_STUTTER\"\n",
    "for i in range(len(x_data)):\n",
    "    x_data[i][IS_STUTTER] = x_data[i]['word'][-1] == '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r\"'\").match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_SUBTREE = \"SIZE_SUBTREE\"\n",
    "NUM_SIBLINGS = \"NUM_SIBLINGS\"\n",
    "# Because we can't assume that every turn contains a fully connected dependency parse, we can treat distance as either 1, 2, or 3 (meaning 2 or more)\n",
    "# dist = 0 when this term is last term\n",
    "# dist = 1 when next term is head or a child of current term\n",
    "# else dist = 2 when there is overlap between (current term's head and children) and (next term's head and children)\n",
    "# else dist = 3\n",
    "ARC_DIST_NEXT = \"ARC_DIST_NEXT\"\n",
    "FUNC = \"FUNC\"\n",
    "def subtree_size(token):\n",
    "    return sum([subtree_size(child) for child in token.children])\n",
    "def num_siblings(token):\n",
    "    return len(list(token.head.children))\n",
    "def arc_dist(token, next_token):\n",
    "    if not next_token: return 0\n",
    "    elif next_token.text == token.head.text or next_token.text in [a.text for a in token.children]: return 1\n",
    "    elif any([a.text in [b.text for b in list(token.children)+[token.head]] for a in list(next_token.children)+[next_token.head]]): return 2\n",
    "    else: return 3\n",
    "def get_func(token):\n",
    "    if \"SUBJ\" in token.dep_: return 0\n",
    "    elif \"DOBJ\" in token.dep_: return 1\n",
    "    elif \"POBJ\" in token.dep_: return 2\n",
    "    else: return 3\n",
    "# set these features to 0 by default\n",
    "for i in range(len(x_data)):\n",
    "    x_data[i][SIZE_SUBTREE] = '-1'\n",
    "    x_data[i][NUM_SIBLINGS] = '-1'\n",
    "    x_data[i][ARC_DIST_NEXT] = '-1'\n",
    "    x_data[i][FUNC] = '-1'\n",
    "for i in range(len(x_data)):\n",
    "    if x_data[i]['word_number_in_turn'] == '1':\n",
    "        # figure out the current turn\n",
    "        turn = ' '.join([word['word'] for word in x_data[i:i+int(x_data[i]['total_number_of_words_in_turn'])]])\n",
    "#         print(turn)\n",
    "        doc = nlp(turn)\n",
    "        for j, token in enumerate(doc):\n",
    "#             print(token)\n",
    "            next_token = doc[i+1] if i+1<len(doc) else None\n",
    "            x_data[i+j][SIZE_SUBTREE] = str(subtree_size(token))\n",
    "            x_data[i+j][NUM_SIBLINGS] = str(num_siblings(token))\n",
    "            x_data[i+j][ARC_DIST_NEXT] = str(arc_dist(token, next_token))\n",
    "            x_data[i+j][FUNC] = str(get_func(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### <class 'sklearn.naive_bayes.MultinomialNB'> #########################################\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   39.8s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 68.143s\n",
      "Best score: 0.704\n",
      "\tclf__alpha: 0.01\n",
      "\tclf__fit_prior: True\n",
      "\tselector__percentile: 90\n",
      "################################### <class 'sklearn.svm.classes.LinearSVC'> #########################################\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 263.686s\n",
      "Best score: 0.746\n",
      "\tclf__C: 1\n",
      "\tclf__loss: 'squared_hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "\tselector__percentile: 90\n",
      "################################### <class 'sklearn.linear_model.logistic.LogisticRegression'> #########################################\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 325.306s\n",
      "Best score: 0.754\n",
      "\tclf__C: 1\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l2'\n",
      "\tselector__percentile: 90\n"
     ]
    }
   ],
   "source": [
    "for (clf, param_grid) in clf_map:\n",
    "    classify_my_model(clf, param_grid, 'syntactic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to use phrase breaks of 4 to segment utterances instead of the turn labels. \n",
    "indices_of_4 = set()\n",
    "with open(file_name, 'r') as f:\n",
    "    for i, l in enumerate(csv.reader(f)):\n",
    "        if i == 0: continue\n",
    "        if l[label_index] == \"4\":# or l[label_index] == \"4-\" or l[label_index] == \"4p\"\n",
    "            indices_of_4.add(i-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set these features to 0 by default\n",
    "for i in range(len(x_data)):\n",
    "    x_data[i][SIZE_SUBTREE] = '-1'\n",
    "    x_data[i][NUM_SIBLINGS] = '-1'\n",
    "    x_data[i][ARC_DIST_NEXT] = '-1'\n",
    "    x_data[i][FUNC] = '-1'\n",
    "sentence = []\n",
    "start_index = 0\n",
    "for i in range(len(x_data)):    \n",
    "    sentence.append(x_data[i]['word'])\n",
    "    if i-1 in indices_of_4:\n",
    "        #we've reached the end of a sentence\n",
    "        doc = nlp(' '.join(sentence))\n",
    "        for j, token in enumerate(doc):\n",
    "            next_token = doc[start_index+1] if start_index+1<len(doc) else None\n",
    "            x_data[start_index+j][SIZE_SUBTREE] = str(subtree_size(token))\n",
    "            x_data[start_index+j][NUM_SIBLINGS] = str(num_siblings(token))\n",
    "            x_data[start_index+j][ARC_DIST_NEXT] = str(arc_dist(token, next_token))\n",
    "            x_data[start_index+j][FUNC] = str(get_func(token))\n",
    "        start_index = i\n",
    "        sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### <class 'sklearn.naive_bayes.MultinomialNB'> #########################################\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:   56.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 57.815s\n",
      "Best score: 0.715\n",
      "\tclf__alpha: 0.01\n",
      "\tclf__fit_prior: True\n",
      "\tselector__percentile: 90\n",
      "################################### <class 'sklearn.svm.classes.LinearSVC'> #########################################\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 190.287s\n",
      "Best score: 0.799\n",
      "\tclf__C: 1\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "\tselector__percentile: 100\n",
      "################################### <class 'sklearn.linear_model.logistic.LogisticRegression'> #########################################\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 283.761s\n",
      "Best score: 0.805\n",
      "\tclf__C: 1\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l1'\n",
      "\tselector__percentile: 100\n"
     ]
    }
   ],
   "source": [
    "for (clf, param_grid) in clf_map:\n",
    "    classify_my_model(clf, param_grid, 'syntactic_punct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze this new model, since it seems to do somewhat better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most significant features\n",
    "model = pickle.load(open('syntactic_punct_best_LogisticRegression.pkl', 'rb'))\n",
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n",
    "def show_least_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names), key=lambda x: abs(x[0]))\n",
    "    top = coefs_with_fns[:n]\n",
    "    for (coef_1, fn_1) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\" % (coef_1, fn_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-3.9139\tword=trying    \t\t5.9484\tPOS_TURN_BIGRAM_RIGHT=VBN/END\n",
      "\t-3.8890\tIS_STUTTER     \t\t5.7678\tPOS_TURN_BIGRAM_RIGHT=NN/END\n",
      "\t-3.4406\tword=parallel  \t\t5.4133\tPOS_TURN_BIGRAM_RIGHT=VBG/END\n",
      "\t-3.2614\tword=an        \t\t5.4095\tPOS_TURN_BIGRAM_RIGHT=PRP/END\n",
      "\t-3.0772\tword=gonna     \t\t5.4017\tPOS_TURN_BIGRAM_RIGHT=VBP/END\n",
      "\t-2.7705\tPOS_TURN_BIGRAM_RIGHT=NNP/NNP\t\t5.2641\tPOS_TURN_BIGRAM_RIGHT=RB/END\n",
      "\t-2.7675\tword=kind      \t\t5.1970\tPOS_TURN_BIGRAM_RIGHT=NNS/END\n",
      "\t-2.6617\tPOS_TURN_BIGRAM_RIGHT=CD/NNS\t\t5.1425\tword=it's-     \n",
      "\t-2.5836\tPOS_TURN_BIGRAM_RIGHT=JJ/NN\t\t5.0438\tPOS_TURN_BIGRAM_RIGHT=JJR/END\n",
      "\t-2.5472\tword=red       \t\t4.9968\tPOS_TURN_BIGRAM_RIGHT=JJ/END\n",
      "\t-2.5011\tPOS_TURN_TRIGRAM=VBD/UH/UH\t\t4.9715\tPOS_TURN_BIGRAM_RIGHT=VB/END\n",
      "\t-2.4398\tPOS_TURN_TRIGRAM=JJ/JJ/UH\t\t4.8884\tPOS_TURN_BIGRAM_RIGHT=VBD/END\n",
      "\t-2.4021\tword_pos_tag=VB_PRP\t\t4.8864\tPOS_TURN_BIGRAM_RIGHT=VBZ/END\n",
      "\t-2.3296\tPOS_TURN_BIGRAM_RIGHT=JJ/TO\t\t4.5564\tPOS_TURN_BIGRAM_RIGHT=NNP/END\n",
      "\t-2.3002\tPOS_TURN_BIGRAM_RIGHT=DT/CD\t\t4.5437\tPOS_TURN_BIGRAM_RIGHT=WRB/END\n",
      "\t-2.2053\tword=directly  \t\t4.5066\tPOS_TURN_BIGRAM_RIGHT=WP/END\n",
      "\t-2.1391\tword=I         \t\t4.4596\tNUM_SIBLINGS=0 \n",
      "\t-2.1238\tword=pretty    \t\t4.3678\tPOS_TURN_BIGRAM_RIGHT=RP/END\n",
      "\t-2.0956\tPOS_TURN_TRIGRAM=JJ/IN/END\t\t4.2460\tPOS_TURN_BIGRAM_RIGHT=UH/END\n",
      "\t-2.0799\tPOS_TURN_BIGRAM_RIGHT=RP/DT\t\t4.1608\tPOS_TURN_BIGRAM_RIGHT=IN/END\n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(model.steps[0][1], model.steps[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_old = pickle.load(open('syntactic_best_LogisticRegression.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-3.0799\tIS_STUTTER     \t\t5.8195\tPOS_TURN_BIGRAM_RIGHT=VBN/END\n",
      "\t-3.0750\tword=kind      \t\t5.6724\tPOS_TURN_BIGRAM_RIGHT=VBP/END\n",
      "\t-2.7437\tPOS_TURN_BIGRAM_RIGHT=PRP/MD\t\t5.6706\tPOS_TURN_BIGRAM_RIGHT=WP/END\n",
      "\t-2.7139\tPOS_TURN_TRIGRAM=RB/like/END\t\t5.4495\tPOS_TURN_BIGRAM_RIGHT=NN/END\n",
      "\t-2.6983\tPOS_TURN_TRIGRAM=RB/there/END\t\t5.2829\tPOS_TURN_BIGRAM_RIGHT=VBD/END\n",
      "\t-2.6206\tPOS_TURN_BIGRAM_RIGHT=NNP/NNP\t\t5.2543\tPOS_TURN_BIGRAM_RIGHT=RB/END\n",
      "\t-2.5483\tword=pretty    \t\t5.1698\tPOS_TURN_BIGRAM_RIGHT=NNS/END\n",
      "\t-2.4947\tPOS_TURN_BIGRAM_RIGHT=VBP_RB/VB\t\t5.1630\tword=it's-     \n",
      "\t-2.3827\tword=red       \t\t5.0925\tPOS_TURN_BIGRAM_RIGHT=VB/END\n",
      "\t-2.3754\tword_pos_tag=VB_PRP\t\t5.0757\tPOS_TURN_BIGRAM_RIGHT=PRP/END\n",
      "\t-2.3681\tPOS_TURN_BIGRAM_RIGHT=VBZ/RP\t\t4.9856\tPOS_TURN_BIGRAM_RIGHT=JJ/END\n",
      "\t-2.3253\tPOS_TURN_BIGRAM_RIGHT=JJ/TO\t\t4.9715\tPOS_TURN_BIGRAM_RIGHT=VBG/END\n",
      "\t-2.2764\tPOS_TURN_TRIGRAM=NN/just/END\t\t4.9332\tPOS_TURN_BIGRAM_RIGHT=VBZ/END\n",
      "\t-2.2459\tPOS_TURN_BIGRAM_RIGHT=JJ/NN\t\t4.7349\tPOS_TURN_BIGRAM_RIGHT=JJR/END\n",
      "\t-2.2370\tword=an        \t\t4.3273\tPOS_TURN_BIGRAM_RIGHT=NNP/END\n",
      "\t-2.2025\tPOS_TURN_BIGRAM_RIGHT=CD/NNS\t\t4.2601\tPOS_TURN_BIGRAM_RIGHT=WRB/END\n",
      "\t-2.1600\tword=I         \t\t4.2519\tPOS_TURN_BIGRAM_RIGHT=RP/END\n",
      "\t-2.0993\tPOS_TURN_BIGRAM_RIGHT=DT/CD\t\t4.1807\tPOS_TURN_BIGRAM_RIGHT=UH/END\n",
      "\t-2.0669\tPOS_TURN_TRIGRAM=NN/like/END\t\t3.9180\tPOS_TURN_BIGRAM_RIGHT=IN/END\n",
      "\t-2.0166\tPOS_TURN_BIGRAM_RIGHT=PRP$/NN\t\t3.8158\tPOS_TURN_BIGRAM_RIGHT=CC/END\n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(model_old.steps[0][1], model_old.steps[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
