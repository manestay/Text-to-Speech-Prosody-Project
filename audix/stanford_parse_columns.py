'''
This script contains methods to generate the following columns:
Syntactic function of current word
Parse tree of current word

For previous explicit and implicit mentions, see AddNewColumns.py

'''

import json
import logging
import pandas as pd
import string
from time import sleep
from stanfordcorenlp import StanfordCoreNLP
from OrganizedBigTable import OrganizedBigTable
from example_config import config

TABLE_NAME =  config['new_table_name']
START_SESSION = config['start_session']
END_SESSION = config['end_session']
CORE_NLP_PATH = config['core_nlp_path']
MEMORY = config['java_memory']

FILLERS = set(['um','uh','uh-huh','hm','ah','mm','mmhm'])
ASSIMILATIONS = set(['gonna','gotta','lemme','gimme','wanna','dun','dunno','you`','you`re',
                     'they`', 'they`re'])
# note that 'cannot' is also an assimilation in CoreNLP, but we can ignore for this corpus
# dunno is lemmatized as du n no, so need 'dun' and 'dunno' to combine correctly
# 'you`re' and 'they`re' with backticks are generated by CoreNLP when they end a sentence, we need
# temporarily fixed by adding to this set
HYPHEN = '-'

def generate_column_data(ann):
    parsed = json.loads(ann)
    sentence = parsed['sentences'][0]

    syntactic_functions = []
    parse_trees = []
    word_list = []

    parse_tree = sentence['parse']
    parse_tree = ' '.join(parse_tree.split()) # collapse whitespace

    dep_list = sentence['enhancedPlusPlusDependencies']
    dep_list = clean_dep_list(dep_list)

    # import pdb; pdb.set_trace()
    for i, d in enumerate(dep_list):
        # '''
        # example entry in list:
        # {'dep': 'nsubj', 'governorGloss': 'thought', 'dependent': 3, 'dependentGloss': 'I', 'governor': 4}
        # '''
        # print(d)
        word = d['dependentGloss']
        syntactic = d['dep']
        is_possessive = word == "'s" and syntactic == 'nmod:poss_case'

        # is_tis = word_list and word == 'is' and word_list[-1] == '\'t'
        # is_end_of_assimilation = word_list and (word_list[-1] + word) in ASSIMILATIONS

        # if word_list and has_apostrophe or is_end_of_assimilation or is_tis:
        #     # print(word_list[-1], word)
        #     syntactic_functions[-1] += '_{}'.format(d['dep'])
        #     word_list[-1] += word
        if d['dep'] == 'punct':
            pass
        elif is_possessive:
            syntactic_functions[-1] += '_{}'.format(syntactic)
            word_list[-1] += '{}'.format(d['dependentGloss'])
        else:
            parse_trees.append(parse_tree)
            syntactic_functions.append(syntactic)
            word_list.append(word)

    # import pdb; pdb.set_trace()
    return parse_trees, syntactic_functions, word_list

def clean_dep_list(dep_list):
    '''
    Clean the dep_list by sorting and removing duplicates
    '''
    dep_list.sort(key=lambda x:x['dependent'])
    seen = set()
    cleaned_dep_list = []
    for d in dep_list:
        word_index = d['dependent']
        if word_index not in seen: # do not add words multiple times
            cleaned_dep_list.append(d)
        seen.add(word_index)
    return cleaned_dep_list

def get_sentence_text(sentence):
    return ' '.join([x['word'] for x in sentence['tokens']])

def clean_sentence(sentence):
    '''
    returns the cleaned sentence list, and a list of indices where a word was removed.
    '''
    indices = []
    word_list = []
    # print(sentence.strip().split(' '))
    for i, word in enumerate(sentence.strip().split(' ')):
        original_word = word
        word = word.translate(str.maketrans('','','-()?')) # fix words such as cha-(ir)
        word = word.replace('`', '\'')
        if word and word not in FILLERS and original_word[-1] != HYPHEN:
            word_list.append(word)
        else:
            indices.append(i)
    return word_list, indices

def generate_column_data_mentions(bigtable):
    curr_syntactic = bigtable.df['syntactic_function']

'''
Runs the script to add columns to each session CSV file. Generates one CSV per session; use
CombineSessionTables.py to merge back into one CSV.
'''
def main(table_name='', client=None):
    if not table_name:
        table_name = TABLE_NAME

    props = {'annotators': 'tokenize,ssplit,pos,parse,depparse','outputFormat':'json'}
    local_client = False
    if not client:
        client = StanfordCoreNLP(CORE_NLP_PATH, memory=MEMORY, quiet=True)
        local_client = True
    for session_number in range(START_SESSION, END_SESSION + 1):
        # print('SESSION {}'.format(session_number))
        parse_trees = []
        syntactic_functions = []
        word_list = []
        bigtable = OrganizedBigTable(session_number, table_name)

        sentences = bigtable.get_sentences()
        sentences = [x + '\n' for x in sentences]
        for sentence in sentences:
            ann = client.annotate(sentence, properties=props)
            pts, sfs, wls = generate_column_data(ann)
            parse_trees += pts
            syntactic_functions += sfs
            word_list += wls
            # print(sentence, '||||||', cleaned_sentence)
        parse_tree_tups = [(x, y) for x, y in zip(word_list, parse_trees)]
        syntactic_tups = [(x, y) for x, y in zip(word_list, syntactic_functions)]

        bigtable.addColumnToDataFrame(parse_tree_tups, 'parse_tree')
        bigtable.addColumnToDataFrame(syntactic_tups, 'syntactic_function')
        bigtable.saveToCSV()
    sleep(5)
    if local_client:
        client.close()

if __name__ == '__main__':
    main()
