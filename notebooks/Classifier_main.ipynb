{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to comparison.xlsx for summary of information in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "from time import time\n",
    "import re\n",
    "from spherecluster import SphericalKMeans\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main functions\n",
    "\n",
    "def generate_data(file_name, features, continuous_feats, impute=0):\n",
    "    break_label = 'word_tobi_break_index'\n",
    "    break_set = set([\"4\", \"4-\", \"4p\"])\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    with open(file_name, 'r') as f:\n",
    "        reader = csv.DictReader(f) # DictReader fixes off-by-one error from before\n",
    "        for i, l in enumerate(reader):\n",
    "            feats = {}\n",
    "            for feat in features:\n",
    "                if feat in continuous_feats:\n",
    "                    if not l[feat]:\n",
    "                        feats[feat] = impute\n",
    "                    else:\n",
    "                        feats[feat] = int(l[feat])\n",
    "                else:\n",
    "                    feats[feat] = l[feat]\n",
    "#             feats = {feat: l[feat] for feat in features}\n",
    "#             # convert some to continuous features\n",
    "#             for feat in continuous_feats:\n",
    "#                 feats[feat] = float(feats[feat])\n",
    "            x_data.append(feats)\n",
    "            label = l[break_label] in break_set\n",
    "            y_data.append(label)\n",
    "    return x_data, y_data\n",
    "\n",
    "def classify_my_model(pipeline, param_grid, X, y, model_name, scorer='f1', save_model=False):\n",
    "    print('#'*35, model_name, '#'*35)\n",
    "    folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=2557)\n",
    "    \n",
    "    gs = GridSearchCV(pipeline,\n",
    "                      param_grid,\n",
    "                      scoring=scorer,\n",
    "                      cv=5,\n",
    "                      n_jobs=-1,\n",
    "                      verbose=1)\n",
    "    t0 = time()\n",
    "    gs.fit(X, y)\n",
    "    train_time = time() - t0\n",
    "    print(\"Train time: %0.3fs\" % train_time)\n",
    "#     print(\"Real train time: %0.3fs\" % (train_time * (TOTAL_COUNT/DEV_COUNT)))\n",
    "    print(\"Best score: %0.3f\" % gs.best_score_)\n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_params[param_name]))\n",
    "    \n",
    "    if save_model:\n",
    "        file_name = model_name + '_best' + \".pkl\"\n",
    "        print('saving model to' + file_name)\n",
    "        with open(file_name, 'wb') as handle:\n",
    "            pickle.dump(best_model, handle)\n",
    "            \n",
    "    return gs.best_score_, gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of classifier models\n",
    "\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "fdot25_scorer = make_scorer(fbeta_score, beta=.25)\n",
    "dict_vectorizer = DictVectorizer()\n",
    "# select_percentile = SelectPercentile(percentile=100)\n",
    "\n",
    "# list of classifier models\n",
    "# redefine for quicker running\n",
    "clf_map = [\n",
    "    (\n",
    "        LinearSVC(),\n",
    "        {\n",
    "            'clf__C': [.1],\n",
    "            'clf__penalty': ['l2'],\n",
    "            'clf__loss': ['hinge', 'squared_hinge'],\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        LogisticRegression(),\n",
    "        {\n",
    "            'clf__penalty': ['l2'],\n",
    "            'clf__fit_intercept': [True],\n",
    "            'clf__C':[.1],\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        RandomForestClassifier(random_state=2557),\n",
    "        {\n",
    "            'clf__n_estimators': [50],\n",
    "            'clf__max_features': [\"auto\", \"log2\"]\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "#     (\n",
    "#         ExtraTreesClassifier(random_state=2557),\n",
    "#         {\n",
    "#             'clf__n_estimators': [20],\n",
    "#             'clf__max_features': [\"auto\", \"log2\"]\n",
    "#         }\n",
    "#     ),\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "FILE_NAME = \"../info-status/games-data-20180427.csv\"\n",
    "df = pd.read_csv(FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'word',\n",
    "#     'word_pos_tag',\n",
    "#     'word_pos_tag_simplified',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_turn',\n",
    "    'word_number_in_task',\n",
    "    'total_number_of_words_in_turn',\n",
    "    'total_number_of_words_in_task',\n",
    "    'Stanford_PoS'\n",
    "]\n",
    "\n",
    "continuous_feats = ['word_number_of_syllables',]\n",
    "\n",
    "x_data, y_data = generate_data(FILE_NAME, features, continuous_feats)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.83      1.00      0.90     11618\n",
      "       True       0.00      0.00      0.00      2448\n",
      "\n",
      "avg / total       0.68      0.83      0.75     14066\n",
      "\n",
      "0.8259633157969573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# majority class baseline classifier\n",
    "# no features are used, which is why we get 0.00 for everything\n",
    "y_pred = [0] * len(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### LinearSVC ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    3.1s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 5.657s\n",
      "Best score: 0.668\n",
      "\tclf__C: 0.1\n",
      "\tclf__loss: 'squared_hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.88      0.97      0.92     11618\n",
      "       True       0.70      0.38      0.49      2448\n",
      "\n",
      "avg / total       0.85      0.86      0.85     14066\n",
      "\n",
      "################################### LogisticRegression ###################################\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 3.465s\n",
      "Best score: 0.678\n",
      "\tclf__C: 0.1\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.88      0.97      0.92     11618\n",
      "       True       0.73      0.34      0.46      2448\n",
      "\n",
      "avg / total       0.85      0.86      0.84     14066\n",
      "\n",
      "################################### RandomForestClassifier ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   23.7s remaining:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   39.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 56.350s\n",
      "Best score: 0.666\n",
      "\tclf__max_features: 'log2'\n",
      "\tclf__n_estimators: 20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.87      0.98      0.92     11618\n",
      "       True       0.75      0.29      0.42      2448\n",
      "\n",
      "avg / total       0.85      0.86      0.83     14066\n",
      "\n",
      "################################### ExtraTreesClassifier ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   32.7s remaining:   21.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   48.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 71.701s\n",
      "Best score: 0.665\n",
      "\tclf__max_features: 'log2'\n",
      "\tclf__n_estimators: 20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.87      0.98      0.92     11618\n",
      "       True       0.74      0.33      0.45      2448\n",
      "\n",
      "avg / total       0.85      0.86      0.84     14066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "    #         ('selector', select_percentile),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train, clf.__class__.__name__, scorer=fdot25_scorer)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with syntactic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### LinearSVC ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    3.9s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 6.851s\n",
      "Best score: 0.674\n",
      "\tclf__C: 0.1\n",
      "\tclf__loss: 'squared_hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.89      0.96      0.92     11618\n",
      "       True       0.71      0.42      0.53      2448\n",
      "\n",
      "avg / total       0.86      0.87      0.86     14066\n",
      "\n",
      "################################### LogisticRegression ###################################\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.2s remaining:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 4.463s\n",
      "Best score: 0.687\n",
      "\tclf__C: 0.1\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.88      0.97      0.92     11618\n",
      "       True       0.72      0.39      0.50      2448\n",
      "\n",
      "avg / total       0.85      0.87      0.85     14066\n",
      "\n",
      "################################### RandomForestClassifier ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   24.1s remaining:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   34.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 51.015s\n",
      "Best score: 0.680\n",
      "\tclf__max_features: 'log2'\n",
      "\tclf__n_estimators: 20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.87      0.97      0.92     11618\n",
      "       True       0.73      0.32      0.45      2448\n",
      "\n",
      "avg / total       0.85      0.86      0.84     14066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    'word',\n",
    "#     'word_pos_tag',\n",
    "#     'word_pos_tag_simplified',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_turn',\n",
    "    'word_number_in_task',\n",
    "    'total_number_of_words_in_turn',\n",
    "    'total_number_of_words_in_task',\n",
    "    'Stanford_PoS',\n",
    "    'syntactic_function'\n",
    "]\n",
    "\n",
    "continuous_feats = ['word_number_of_syllables',]\n",
    "\n",
    "x_data, y_data = generate_data(FILE_NAME, features, continuous_feats)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)\n",
    "\n",
    "    \n",
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "    #         ('selector', select_percentile),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train, clf.__class__.__name__, scorer=fdot25_scorer)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with syntactic features and basic mention features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### LinearSVC ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    6.4s remaining:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 10.064s\n",
      "Best score: 0.684\n",
      "\tclf__C: 0.1\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.87      0.97      0.92     11618\n",
      "       True       0.73      0.33      0.46      2448\n",
      "\n",
      "avg / total       0.85      0.86      0.84     14066\n",
      "\n",
      "################################### LogisticRegression ###################################\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    4.3s remaining:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 8.519s\n",
      "Best score: 0.690\n",
      "\tclf__C: 0.1\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.88      0.97      0.92     11618\n",
      "       True       0.72      0.39      0.51      2448\n",
      "\n",
      "avg / total       0.85      0.87      0.85     14066\n",
      "\n",
      "################################### RandomForestClassifier ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   28.9s remaining:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   41.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 60.234s\n",
      "Best score: 0.676\n",
      "\tclf__max_features: 'log2'\n",
      "\tclf__n_estimators: 20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.87      0.98      0.92     11618\n",
      "       True       0.73      0.30      0.43      2448\n",
      "\n",
      "avg / total       0.84      0.86      0.83     14066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    'word',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_turn',\n",
    "    'word_number_in_task',\n",
    "    'total_number_of_words_in_turn',\n",
    "    'total_number_of_words_in_task',\n",
    "    'Stanford_PoS',\n",
    "    'syntactic_function',\n",
    "    'Most_Recent_Mention_Syntactic_Function',\n",
    "    'Recent_Explicit_Mention_Syntactic_Function',\n",
    "    'Recent_Implicit_Mention_Syntactic_Function',\n",
    "    'Most_Recent_Mention_PoS',\n",
    "    'Recent_Explicit_Mention_PoS',\n",
    "    'Recent_Implicit_Mention_PoS',\n",
    "]\n",
    "\n",
    "continuous_feats = ['word_number_of_syllables',]\n",
    "\n",
    "x_data, y_data = generate_data(FILE_NAME, features, continuous_feats)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)\n",
    "\n",
    "    \n",
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "    #         ('selector', select_percentile),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train, clf.__class__.__name__, scorer=fdot25_scorer)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from accent_classifier.ipynb\n",
    "\n",
    "def get_input_mentioned(X_data, df):\n",
    "    col = df['Most_Recent_Mention']\n",
    "    for i in range(len(X_data)):\n",
    "        mentioned = True if not pd.isnull(col[i]) else False\n",
    "        X_data[i]['mentioned'] = mentioned\n",
    "\n",
    "def set_input_num_mentions(X_data, df):\n",
    "    col = df['Number_Of_Coref_Mentions']\n",
    "    for i in range(len(X_data)):\n",
    "        num = col[i] if not pd.isnull(col[i]) else 0\n",
    "        X_data[i]['Number_Of_Coref_Mentions'] = num\n",
    "\n",
    "def get_input_far_back_mentioned(X_data, df):\n",
    "    curr_time_col = df['word_end_time']\n",
    "    most_recent_time_col = df['Most_Recent_Mention']\n",
    "    for i in range(len(X_data)):\n",
    "        curr_time = curr_time_col[i]\n",
    "        most_recent_time = most_recent_time_col[i] if not pd.isnull(most_recent_time_col[i]) else curr_time\n",
    "        X_data[i]['time_between_mentions'] = curr_time - most_recent_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### LinearSVC ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   29.5s remaining:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   39.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 51.161s\n",
      "Best score: 0.669\n",
      "\tclf__C: 0.1\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.87      0.97      0.92     11618\n",
      "       True       0.72      0.34      0.46      2448\n",
      "\n",
      "avg / total       0.85      0.86      0.84     14066\n",
      "\n",
      "################################### LogisticRegression ###################################\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    5.3s remaining:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    6.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 9.999s\n",
      "Best score: 0.687\n",
      "\tclf__C: 0.1\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.88      0.97      0.92     11618\n",
      "       True       0.72      0.39      0.51      2448\n",
      "\n",
      "avg / total       0.85      0.87      0.85     14066\n",
      "\n",
      "################################### RandomForestClassifier ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   28.5s remaining:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   37.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 55.504s\n",
      "Best score: 0.684\n",
      "\tclf__max_features: 'auto'\n",
      "\tclf__n_estimators: 20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.88      0.97      0.92     11618\n",
      "       True       0.72      0.35      0.47      2448\n",
      "\n",
      "avg / total       0.85      0.86      0.84     14066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    'word',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_turn',\n",
    "    'word_number_in_task',\n",
    "    'total_number_of_words_in_turn',\n",
    "    'total_number_of_words_in_task',\n",
    "    'Stanford_PoS',\n",
    "    'syntactic_function',\n",
    "    'Most_Recent_Mention_Syntactic_Function',\n",
    "    'Recent_Explicit_Mention_Syntactic_Function',\n",
    "    'Recent_Implicit_Mention_Syntactic_Function',\n",
    "    'Most_Recent_Mention_PoS',\n",
    "    'Recent_Explicit_Mention_PoS',\n",
    "    'Recent_Implicit_Mention_PoS',\n",
    "]\n",
    "\n",
    "continuous_feats = ['word_number_of_syllables']\n",
    "\n",
    "x_data, y_data = generate_data(FILE_NAME, features, continuous_feats)\n",
    "get_input_mentioned(x_data, df)\n",
    "set_input_num_mentions(x_data, df)\n",
    "get_input_far_back_mentioned(x_data, df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)\n",
    "\n",
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train, clf.__class__.__name__, scorer=fdot25_scorer)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding POS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dist_end_turn(x_data):\n",
    "    # distance from end of turn\n",
    "    DIST_END_TURN = \"DIST_END_TURN\"\n",
    "    for i in range(len(x_data)):\n",
    "        x_data[i][DIST_END_TURN] = int(x_data[i]['total_number_of_words_in_turn']) - int(x_data[i]['word_number_in_turn'])\n",
    "        \n",
    "def add_pos_bigram(x_data, left=True, right=True):\n",
    "    POS_TURN_BIGRAM_LEFT = \"POS_TURN_BIGRAM_LEFT\"\n",
    "    POS_TURN_BIGRAM_RIGHT = \"POS_TURN_BIGRAM_RIGHT\"\n",
    "    \n",
    "    def add_left_bigram(x_data):\n",
    "        if x_data[i]['word_number_in_turn'] == '1':\n",
    "            x_data[i][POS_TURN_BIGRAM_LEFT] = 'BEGIN/'+x_data[i]['Stanford_PoS']\n",
    "        else:\n",
    "            x_data[i][POS_TURN_BIGRAM_LEFT] = x_data[i-1]['Stanford_PoS']+\"/\"+x_data[i]['Stanford_PoS']\n",
    "    def add_right_bigram(x_data):\n",
    "        word_number, total_word_number = int(x_data[i]['word_number_in_turn']), int(x_data[i]['total_number_of_words_in_turn'])\n",
    "        if word_number == total_word_number:\n",
    "            x_data[i][POS_TURN_BIGRAM_RIGHT] = x_data[i]['Stanford_PoS']+\"/END\"\n",
    "        else:\n",
    "            x_data[i][POS_TURN_BIGRAM_RIGHT] = x_data[i]['Stanford_PoS']+\"/\"+x_data[i+1]['Stanford_PoS']\n",
    "            \n",
    "    if not left: add_left_bigram = lambda x: None\n",
    "    if not right: add_right_bigram = lambda x: None\n",
    "        \n",
    "    for i in range(len(x_data)):\n",
    "        add_left_bigram(x_data)\n",
    "        add_right_bigram(x_data)\n",
    "        \n",
    "# not super useful in my experiments, kept for reference\n",
    "def add_word_bigram(x_data, left=True, right=True):\n",
    "    POS_TURN_BIGRAM_LEFT = \"POS_WORD_BIGRAM_LEFT\"\n",
    "    POS_TURN_BIGRAM_RIGHT = \"POS_WORD_BIGRAM_RIGHT\"\n",
    "    \n",
    "    def add_left_bigram(x_data):\n",
    "        if x_data[i]['word_number_in_turn'] == '1':\n",
    "            x_data[i][POS_TURN_BIGRAM_LEFT] = 'BEGIN/'+x_data[i]['word']\n",
    "        else:\n",
    "            x_data[i][POS_TURN_BIGRAM_LEFT] = x_data[i-1]['word']+\"/\"+x_data[i]['word']\n",
    "    def add_right_bigram(x_data):\n",
    "        word_number, total_word_number = int(x_data[i]['word_number_in_turn']), int(x_data[i]['total_number_of_words_in_turn'])\n",
    "        if word_number == total_word_number:\n",
    "            x_data[i][POS_TURN_BIGRAM_RIGHT] = x_data[i]['word']+\"/END\"\n",
    "        else:\n",
    "            x_data[i][POS_TURN_BIGRAM_RIGHT] = x_data[i]['word']+\"/\"+x_data[i+1]['word']\n",
    "            \n",
    "    if not left: add_left_bigram = lambda x: None\n",
    "    if not right: add_right_bigram = lambda x: None\n",
    "        \n",
    "    for i in range(len(x_data)):\n",
    "        add_left_bigram(x_data)\n",
    "        add_right_bigram(x_data)\n",
    "        \n",
    "def add_pos_trigram(x_data):\n",
    "    POS_TURN_TRIGRAM = \"POS_TURN_TRIGRAM\"\n",
    "    for i in range(len(x_data)):\n",
    "        # to the left\n",
    "        left = \"BEGIN\"\n",
    "        if x_data[i]['word_number_in_turn'] != '1':\n",
    "            left = x_data[i-1]['Stanford_PoS']\n",
    "        # to the right\n",
    "        right = \"END\"\n",
    "        word_number, total_word_number = int(x_data[i]['word_number_in_turn']), int(x_data[i]['total_number_of_words_in_turn'])\n",
    "        if x_data[i]['word_number_in_turn'] != x_data[i]['total_number_of_words_in_turn']:\n",
    "            right = x_data[i+1]['Stanford_PoS']\n",
    "        x_data[i][POS_TURN_TRIGRAM] = left+\"/\"+x_data[i]['Stanford_PoS']+\"/\"+right\n",
    "        \n",
    "def add_is_stutter(x_data):\n",
    "    IS_STUTTER = \"IS_STUTTER\"\n",
    "    for i in range(len(x_data)):\n",
    "        x_data[i][IS_STUTTER] = x_data[i]['word'][-1] == '-'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With pos bigram features\n",
    "We no longer include the extra mention features, since didn't help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### LinearSVC ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   41.6s remaining:   27.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   53.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 65.750s\n",
      "Best score: 0.732\n",
      "\tclf__C: 0.1\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.91      0.96      0.93     11618\n",
      "       True       0.75      0.53      0.62      2448\n",
      "\n",
      "avg / total       0.88      0.89      0.88     14066\n",
      "\n",
      "################################### LogisticRegression ###################################\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    6.8s remaining:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    8.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 12.776s\n",
      "Best score: 0.741\n",
      "\tclf__C: 0.1\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.90      0.97      0.93     11618\n",
      "       True       0.77      0.51      0.62      2448\n",
      "\n",
      "avg / total       0.88      0.89      0.88     14066\n",
      "\n",
      "################################### RandomForestClassifier ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  1.3min remaining:   52.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 142.963s\n",
      "Best score: 0.734\n",
      "\tclf__max_features: 'log2'\n",
      "\tclf__n_estimators: 20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.88      0.98      0.93     11618\n",
      "       True       0.79      0.38      0.51      2448\n",
      "\n",
      "avg / total       0.87      0.87      0.86     14066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    'word',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_turn',\n",
    "    'word_number_in_task',\n",
    "    'total_number_of_words_in_turn',\n",
    "    'total_number_of_words_in_task',\n",
    "    'Stanford_PoS',\n",
    "    'syntactic_function',\n",
    "    'Most_Recent_Mention_Syntactic_Function',\n",
    "    'Recent_Explicit_Mention_Syntactic_Function',\n",
    "    'Recent_Implicit_Mention_Syntactic_Function',\n",
    "    'Most_Recent_Mention_PoS',\n",
    "    'Recent_Explicit_Mention_PoS',\n",
    "    'Recent_Implicit_Mention_PoS',\n",
    "]\n",
    "\n",
    "continuous_feats = ['word_number_of_syllables']\n",
    "\n",
    "x_data, y_data = generate_data(FILE_NAME, features, continuous_feats)\n",
    "add_dist_end_turn(x_data)\n",
    "add_pos_bigram(x_data)\n",
    "add_word_bigram(x_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)\n",
    "\n",
    "    \n",
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "    #         ('selector', select_percentile),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train, clf.__class__.__name__, scorer=fdot25_scorer)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_number_of_syllables': 1, 'word_number_in_turn': '1', 'DIST_END_TURN': 0, 'Recent_Implicit_Mention_PoS': '', 'POS_TURN_BIGRAM_RIGHT': 'NN/END', 'Recent_Explicit_Mention_Syntactic_Function': '', 'Recent_Explicit_Mention_PoS': '', 'POS_WORD_BIGRAM_RIGHT': 'yup/END', 'POS_TURN_BIGRAM_LEFT': 'BEGIN/NN', 'Stanford_PoS': 'NN', 'total_number_of_words_in_turn': '1', 'word_number_in_task': '1', 'syntactic_function': 'ROOT', 'Most_Recent_Mention_Syntactic_Function': '', 'total_number_of_words_in_task': '50', 'Recent_Implicit_Mention_Syntactic_Function': '', 'word': 'yup', 'POS_WORD_BIGRAM_LEFT': 'BEGIN/yup', 'Most_Recent_Mention_PoS': ''}\n"
     ]
    }
   ],
   "source": [
    "print(x_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With pos bigram features, with is_stutter\n",
    "Note: pos trigrams were tested, but did not help accuracy. Given both right and left bigrams, trigrams may not capture any additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### LinearSVC ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   32.0s remaining:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   41.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 51.409s\n",
      "Best score: 0.732\n",
      "\tclf__C: 0.1\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.91      0.96      0.93     11618\n",
      "       True       0.75      0.53      0.62      2448\n",
      "\n",
      "avg / total       0.88      0.89      0.88     14066\n",
      "\n",
      "0.8876013081188682\n",
      "################################### LogisticRegression ###################################\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    7.0s remaining:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    8.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 13.845s\n",
      "Best score: 0.740\n",
      "\tclf__C: 0.1\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.90      0.97      0.93     11618\n",
      "       True       0.77      0.52      0.62      2448\n",
      "\n",
      "avg / total       0.88      0.89      0.88     14066\n",
      "\n",
      "0.8885966159533627\n",
      "################################### RandomForestClassifier ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  1.5min remaining:   58.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 161.314s\n",
      "Best score: 0.739\n",
      "\tclf__max_features: 'log2'\n",
      "\tclf__n_estimators: 20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.88      0.98      0.93     11618\n",
      "       True       0.77      0.39      0.52      2448\n",
      "\n",
      "avg / total       0.86      0.87      0.86     14066\n",
      "\n",
      "0.8743068391866913\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    'word',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_turn',\n",
    "    'word_number_in_task',\n",
    "    'total_number_of_words_in_turn',\n",
    "    'total_number_of_words_in_task',\n",
    "    'Stanford_PoS',\n",
    "    'syntactic_function',\n",
    "    'Most_Recent_Mention_Syntactic_Function',\n",
    "    'Recent_Explicit_Mention_Syntactic_Function',\n",
    "    'Recent_Implicit_Mention_Syntactic_Function',\n",
    "    'Most_Recent_Mention_PoS',\n",
    "    'Recent_Explicit_Mention_PoS',\n",
    "    'Recent_Implicit_Mention_PoS',\n",
    "]\n",
    "\n",
    "continuous_feats = ['word_number_of_syllables']\n",
    "\n",
    "x_data, y_data = generate_data(FILE_NAME, features, continuous_feats)\n",
    "add_dist_end_turn(x_data)\n",
    "add_pos_bigram(x_data)\n",
    "add_word_bigram(x_data)\n",
    "add_is_stutter(x_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)\n",
    "\n",
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "    #         ('selector', select_percentile),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train, clf.__class__.__name__, scorer=fdot25_scorer)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding word embedding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G Word embeddings: 1193514\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_FILE = \"/mnt/e/word2vec/glove.twitter.27B.200d.txt\"\n",
    "# EMBEDDING_DIM = 50\n",
    "#load embeddings\n",
    "gembeddings_index = {}\n",
    "with open(EMBEDDINGS_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        gembedding = np.asarray(values[1:], dtype='float32')\n",
    "        gembeddings_index[word] = gembedding\n",
    "\n",
    "print('G Word embeddings:', len(gembeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings(x_data, embeddings_d):\n",
    "    dim_len = len(embeddings_d['the'])\n",
    "    for i in range(len(x_data)):\n",
    "        vector = [0] * dim_len\n",
    "        word = x_data[i]['word']\n",
    "        if word in embeddings_d:\n",
    "            vector = embeddings_d[word]\n",
    "        \n",
    "        for dim in range(dim_len):\n",
    "            x_data[i]['embedding_dim{}'.format(dim)] = vector[dim]\n",
    "\n",
    "\n",
    "def add_embedding_cluster(x_data, embeddings_d, sphere=True, clusters=10):\n",
    "    dim_len = len(embeddings_d['the'])\n",
    "    x_words = [x['word'].lower() for x in x_data]\n",
    "    embeddings = [embeddings_d.get(word) if word in embeddings_d else [0.0] * dim_len for word in x_words]\n",
    "    \n",
    "    if sphere: # equivalent to cosine similarity\n",
    "        print('doing spherical kmeans')\n",
    "        kmeans = SphericalKMeans(n_clusters=clusters, random_state=2557).fit(embeddings)\n",
    "    else: # Euclidean distance\n",
    "        kmeans = KMeans(n_clusters=clusters, random_state=2557).fit(embeddings)\n",
    "\n",
    "    for i in range(len(x_data)):\n",
    "        x_data[i]['embedding_cluster'] = str(kmeans.labels_[i])\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing spherical kmeans\n",
      "{'word_number_of_syllables': 1, 'word_number_in_turn': '1', 'word_number_in_task': '1', 'total_number_of_words_in_task': '50', 'total_number_of_words_in_turn': '1', 'word': 'yup', 'embedding_cluster': 38, 'Stanford_PoS': 'NN'}\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    'word',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_turn',\n",
    "    'word_number_in_task',\n",
    "    'total_number_of_words_in_turn',\n",
    "    'total_number_of_words_in_task',\n",
    "    'Stanford_PoS',\n",
    "]\n",
    "\n",
    "continuous_feats = [\n",
    "    'word_number_of_syllables',\n",
    "#     'word_number_in_turn',\n",
    "#     'word_number_in_task',\n",
    "#     'total_number_of_words_in_turn',\n",
    "#     'total_number_of_words_in_task',\n",
    "]\n",
    "\n",
    "x_data, y_data = generate_data(FILE_NAME, features, continuous_feats)\n",
    "# add_dist_end_turn(x_data)\n",
    "# add_pos_bigram(x_data)\n",
    "# add_is_stutter(x_data)\n",
    "\n",
    "# get_input_mentioned(x_data)\n",
    "# set_input_num_mentions(x_data)\n",
    "# get_input_far_back_mentioned(x_data)\n",
    "\n",
    "NUM_CLUSTERS=50\n",
    "SPHERE=True\n",
    "kmeans = add_embedding_cluster(x_data, gembeddings_index, sphere=SPHERE, clusters=NUM_CLUSTERS)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)\n",
    "print(x_data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWITTER 200 CLUSTER-50\n",
      "################################### LinearSVC ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   25.1s remaining:   16.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   30.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 42.392s\n",
      "Best score: 0.667\n",
      "\tclf__C: 0.1\n",
      "\tclf__loss: 'squared_hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.88      0.97      0.92     11618\n",
      "       True       0.70      0.38      0.49      2448\n",
      "\n",
      "avg / total       0.85      0.86      0.85     14066\n",
      "\n",
      "0.8637850135077492\n",
      "################################### LogisticRegression ###################################\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    5.2s remaining:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    7.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 11.345s\n",
      "Best score: 0.678\n",
      "\tclf__C: 0.1\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.88      0.97      0.92     11618\n",
      "       True       0.73      0.34      0.46      2448\n",
      "\n",
      "avg / total       0.85      0.86      0.84     14066\n",
      "\n",
      "0.8631451727570028\n",
      "################################### RandomForestClassifier ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   25.9s remaining:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   35.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 52.696s\n",
      "Best score: 0.667\n",
      "\tclf__max_features: 'log2'\n",
      "\tclf__n_estimators: 20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.87      0.98      0.92     11618\n",
      "       True       0.72      0.30      0.42      2448\n",
      "\n",
      "avg / total       0.84      0.86      0.83     14066\n",
      "\n",
      "0.857457699417034\n"
     ]
    }
   ],
   "source": [
    "print(\"TWITTER 200 CLUSTER-50\")\n",
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train, clf.__class__.__name__, scorer=fdot25_scorer)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Syntactic Features\n",
    "Not extensively tested -- future work should begin here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_number_in_turn': '1', 'POS_WORD_BIGRAM_RIGHT': 'yup/END', 'tree_depth': 4, 'constituent_backward_position': 0, 'Recent_Explicit_Mention_PoS': '', 'Stanford_PoS': 'NN', 'constituent_width': 1, 'constituent_label': 'NP', 'syntactic_function': 'ROOT', 'Most_Recent_Mention_Syntactic_Function': '', 'Most_Recent_Mention_PoS': '', 'Recent_Implicit_Mention_Syntactic_Function': '', 'POS_WORD_BIGRAM_LEFT': 'BEGIN/yup', 'word_number_of_syllables': 1, 'DIST_END_TURN': 0, 'POS_TURN_BIGRAM_RIGHT': 'NN/END', 'total_number_of_words_in_turn': '1', 'Recent_Implicit_Mention_PoS': '', 'constituent_forward_position': 0, 'word_depth': 3, 'Recent_Explicit_Mention_Syntactic_Function': '', 'word_number_in_task': '1', 'POS_TURN_BIGRAM_LEFT': 'BEGIN/NN', 'total_number_of_words_in_task': '50', 'word': 'yup', 'tree_width': 1, 'IS_STUTTER': False}\n",
      "################################### LinearSVC ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   39.0s remaining:   26.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   51.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 67.801s\n",
      "Best score: 0.728\n",
      "\tclf__C: 0.1\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "saving model toLinearSVC_best.pkl\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.91      0.96      0.93     11618\n",
      "       True       0.75      0.53      0.62      2448\n",
      "\n",
      "avg / total       0.88      0.89      0.88     14066\n",
      "\n",
      "0.8880989620361155\n",
      "################################### LogisticRegression ###################################\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   11.3s remaining:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   13.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 20.460s\n",
      "Best score: 0.739\n",
      "\tclf__C: 0.1\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l2'\n",
      "saving model toLogisticRegression_best.pkl\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.90      0.97      0.93     11618\n",
      "       True       0.76      0.52      0.62      2448\n",
      "\n",
      "avg / total       0.88      0.89      0.88     14066\n",
      "\n",
      "0.887814588369117\n",
      "################################### RandomForestClassifier ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  1.3min remaining:   53.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 145.058s\n",
      "Best score: 0.733\n",
      "\tclf__max_features: 'auto'\n",
      "\tclf__n_estimators: 20\n",
      "saving model toRandomForestClassifier_best.pkl\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.89      0.97      0.93     11618\n",
      "       True       0.77      0.44      0.56      2448\n",
      "\n",
      "avg / total       0.87      0.88      0.87     14066\n",
      "\n",
      "0.8807052466941562\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    'word',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_turn',\n",
    "    'word_number_in_task',\n",
    "    'total_number_of_words_in_turn',\n",
    "    'total_number_of_words_in_task',\n",
    "    'Stanford_PoS',\n",
    "    'syntactic_function',\n",
    "    'Most_Recent_Mention_Syntactic_Function',\n",
    "    'Recent_Explicit_Mention_Syntactic_Function',\n",
    "    'Recent_Implicit_Mention_Syntactic_Function',\n",
    "    'Most_Recent_Mention_PoS',\n",
    "    'Recent_Explicit_Mention_PoS',\n",
    "    'Recent_Implicit_Mention_PoS',\n",
    "    \n",
    "    'tree_depth',\n",
    "    'tree_width',\n",
    "    'word_depth',\n",
    "    'constituent_width',\n",
    "    'constituent_label',\n",
    "    'constituent_forward_position',\n",
    "    'constituent_backward_position'\n",
    "]\n",
    "\n",
    "continuous_feats = [\n",
    "    'word_number_of_syllables',\n",
    "    'tree_depth',\n",
    "    'tree_width',\n",
    "    'word_depth',\n",
    "    'constituent_width',\n",
    "    'constituent_forward_position',\n",
    "    'constituent_backward_position'\n",
    "]\n",
    "x_data, y_data = generate_data(FILE_NAME, features, continuous_feats, impute=-1)\n",
    "add_dist_end_turn(x_data)\n",
    "add_pos_bigram(x_data)\n",
    "add_word_bigram(x_data)\n",
    "add_is_stutter(x_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)\n",
    "\n",
    "print(x_data[0])\n",
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "    #         ('selector', select_percentile),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train,\n",
    "                                               clf.__class__.__name__, scorer=fdot25_scorer,\n",
    "                                               save_model=True)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accent Classifier functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted get_labels() from accent_classifier.ipynb\n",
    "\n",
    "def get_labels(df, x_data=None):\n",
    "    '''\n",
    "    get accent label information from df, and optionally write to x_data\n",
    "    '''\n",
    "    accent_columns = df[['word_tobi_break_index', 'word_tobi_pitch_accent']]\n",
    "    \n",
    "    y_labels = []\n",
    "    \n",
    "    for index, row in accent_columns.iterrows():\n",
    "        \n",
    "        if row['word_tobi_pitch_accent'] == \"*?\":\n",
    "            y_labels.append(0)\n",
    "            \n",
    "        elif row['word_tobi_pitch_accent'] == \"_\":\n",
    "            y_labels.append(0)\n",
    "            \n",
    "        else:\n",
    "            y_labels.append(1)\n",
    "    if x_data:\n",
    "        print(len(y_labels))\n",
    "        for i, label in enumerate(y_labels):\n",
    "            if i >= len(x_data):\n",
    "                break\n",
    "            x_data[i][\"accent_label0\"] = y_labels[i]\n",
    "    return np.array(y_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def iter_window(seq, n=2):\n",
    "    \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "    \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result\n",
    "    for elem in it:\n",
    "        result = result[1:] + (elem,)\n",
    "        yield result\n",
    "\n",
    "def get_label_window(labels, x_data=None, window_size=2, prev=0):\n",
    "    '''\n",
    "    :param labels: list of binary values generated by get_labels()\n",
    "    :param x_data: the data list, if given adds accent0 to accent<window_size> features to each row,\n",
    "                   where x is the index of the accent relative to the current row (ex. accent-1 is\n",
    "                   the accent of the row before)\n",
    "    :param window_size: int size of window\n",
    "    :param prev: int offset of start (ex. prev=1 means looks 1 back)\n",
    "    '''\n",
    "    start_index = 0\n",
    "    labels_windows = []\n",
    "    for w in iter_window(labels, window_size):\n",
    "        labels_windows.append(list(w))\n",
    "    if prev:\n",
    "        start_index = -prev\n",
    "        initial = labels_windows[0]\n",
    "        to_prepend = []\n",
    "        for i in range(prev, 0, -1):\n",
    "            a = ([0] * (i))\n",
    "            a.extend(initial[0:window_size - (i)])\n",
    "            to_prepend.append(a)\n",
    "        labels_windows = to_prepend + labels_windows\n",
    "    if len(labels_windows) < len(labels):\n",
    "        diff = len(labels) - len(labels_windows)\n",
    "        final = labels_windows[-1]\n",
    "        for i in range(diff):\n",
    "            a = final[(i+1):]\n",
    "            a.extend([0] * (i+1))\n",
    "            labels_windows.append(a)\n",
    "    if x_data:\n",
    "        for i in range(len(x_data)):\n",
    "            count = start_index\n",
    "            for j in range(window_size):\n",
    "                x_data[i]['accent{}'.format(count)] = labels_windows[i][j]\n",
    "                count += 1\n",
    "                \n",
    "    labels_windows = np.array(labels_windows)\n",
    "    return labels_windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_number_of_syllables': 1, 'word_number_in_turn': '1', 'DIST_END_TURN': 0, 'accent-1': 0, 'POS_TURN_BIGRAM_RIGHT': 'NN/END', 'accent0': 1, 'POS_TURN_BIGRAM_LEFT': 'BEGIN/NN', 'Stanford_PoS': 'NN', 'total_number_of_words_in_turn': '1', 'word_number_in_task': '1', 'syntactic_function': 'ROOT', 'total_number_of_words_in_task': '50', 'word': 'yup', 'accent1': 1, 'IS_STUTTER': False}\n",
      "everything\n",
      "################################### LinearSVCpitch_accent_gold ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   26.4s remaining:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   32.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 42.503s\n",
      "Best score: 0.741\n",
      "\tclf__C: 0.1\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "saving model toLinearSVCpitch_accent_gold_best.pkl\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.91      0.97      0.93     11618\n",
      "       True       0.76      0.52      0.62      2448\n",
      "\n",
      "avg / total       0.88      0.89      0.88     14066\n",
      "\n",
      "0.8886677093701123\n",
      "################################### LogisticRegressionpitch_accent_gold ###################################\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    5.9s remaining:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    8.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 12.950s\n",
      "Best score: 0.745\n",
      "\tclf__C: 0.1\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l2'\n",
      "saving model toLogisticRegressionpitch_accent_gold_best.pkl\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.91      0.97      0.94     11618\n",
      "       True       0.77      0.52      0.62      2448\n",
      "\n",
      "avg / total       0.88      0.89      0.88     14066\n",
      "\n",
      "0.8893786435376084\n",
      "################################### RandomForestClassifierpitch_accent_gold ###################################\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   57.1s remaining:   38.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 119.580s\n",
      "Best score: 0.820\n",
      "\tclf__max_features: 'auto'\n",
      "\tclf__n_estimators: 50\n",
      "saving model toRandomForestClassifierpitch_accent_gold_best.pkl\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.91      0.98      0.94     11618\n",
      "       True       0.87      0.53      0.65      2448\n",
      "\n",
      "avg / total       0.90      0.90      0.89     14066\n",
      "\n",
      "0.9035262334707806\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    'word',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_turn',\n",
    "    'word_number_in_task',\n",
    "    'total_number_of_words_in_turn',\n",
    "    'total_number_of_words_in_task',\n",
    "    'Stanford_PoS',\n",
    "    'syntactic_function',\n",
    "    \n",
    "#     'tree_depth',\n",
    "#     'tree_width',\n",
    "#     'word_depth',\n",
    "#     'constituent_width',\n",
    "#     'constituent_label',\n",
    "#     'constituent_forward_position',\n",
    "#     'constituent_backward_position'\n",
    "]\n",
    "\n",
    "continuous_feats = [\n",
    "    'word_number_of_syllables',\n",
    "#     'tree_depth',\n",
    "#     'tree_width',\n",
    "#     'word_depth',\n",
    "#     'constituent_width',\n",
    "#     'constituent_forward_position',\n",
    "#     'constituent_backward_position'\n",
    "]\n",
    "\n",
    "x_data, y_data = generate_data(FILE_NAME, features, continuous_feats)\n",
    "add_dist_end_turn(x_data)\n",
    "add_pos_bigram(x_data)\n",
    "# add_word_bigram(x_data)\n",
    "add_is_stutter(x_data)\n",
    "\n",
    "# get_input_mentioned(x_data, df)\n",
    "# set_input_num_mentions(x_data, df)\n",
    "# get_input_far_back_mentioned(x_data, df)\n",
    "\n",
    "# kmeans = add_embedding_cluster(x_data, gembeddings_index,sphere=True, clusters=50)\n",
    "\n",
    "labels = get_labels(df)\n",
    "get_label_window(labels, x_data=x_data, window_size=3, prev=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)\n",
    "print(x_data[0])\n",
    "\n",
    "print(\"everything\")\n",
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train,\n",
    "                                               clf.__class__.__name__ +'pitch_accent_gold',\n",
    "                                               scorer=fdot25_scorer, save_model=True)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n",
    "def show_least_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names), key=lambda x: abs(x[0]))\n",
    "    top = coefs_with_fns[:n]\n",
    "    for (coef_1, fn_1) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\" % (coef_1, fn_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_most_informative_features(model.steps[0][1], model.steps[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_least_informative_features(model.steps[0][1], model.steps[2][1], n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.steps[2][1].coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def show_feature_importance(vectorizer, clf):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = zip(clf.coef_[0], feature_names)\n",
    "    feat_count = defaultdict(list)\n",
    "    for (coef_1, fn_1) in coefs_with_fns:\n",
    "        feat = fn_1.split('=')[0]\n",
    "        feat_count[feat].append(abs(coef_1))\n",
    "    for feat in feat_count.keys():\n",
    "        print(feat, sum(feat_count[feat])/len(feat_count[feat]))\n",
    "    return feat_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_count = show_feature_importance(model.steps[0][1], model.steps[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for feat in feat_count.keys():\n",
    "    hist, bins = np.histogram(feat_count[feat], bins=int(len(feat_count[feat])**0.5))\n",
    "    width = 0.7 * (bins[1] - bins[0])\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.bar(center, hist, align='center', width=width)\n",
    "    plt.title(feat)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
