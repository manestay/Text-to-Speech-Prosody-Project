{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "from time import time\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "# nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some variables for processing data\n",
    "\n",
    "features = [\n",
    "    'word',\n",
    "    'word_pos_tag',\n",
    "    'word_pos_tag_simplified',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_turn',\n",
    "    'word_number_in_task'\n",
    "]\n",
    "\n",
    "continuous_feats = [\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_turn',\n",
    "    'word_number_in_task'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"big-table-PoS.csv\"\n",
    "\n",
    "def generate_data(file_name, features, continuous_feats):\n",
    "    break_label = 'word_tobi_break_index'\n",
    "    break_set = set([\"4\", \"4-\", \"4p\"])\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    with open(file_name, 'r') as f:\n",
    "        reader = csv.DictReader(f) # DictReader fixes off-by-one error from before\n",
    "        for i, l in enumerate(reader):\n",
    "            feats = {feat: l[feat] for feat in features}\n",
    "            # convert some to continuous features\n",
    "            for feat in continuous_feats:\n",
    "                feats[feat] = float(feats[feat])\n",
    "            x_data.append(feats)\n",
    "            label = l[break_label] in break_set\n",
    "            y_data.append(label)\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = generate_data(\"big-table-PoS.csv\", features, continuous_feats)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_number_of_syllables': 1.0, 'word_pos_tag_simplified': 'O', 'word_number_in_task': 12.0, 'word_pos_tag': 'PRP', 'word_number_in_turn': 2.0, 'word': 'it'} False\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_my_model(pipeline, param_grid, X, y, model_name, scorer='f1'):\n",
    "    print('#'*35, model_name, '#'*35)\n",
    "    folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=int(time()))\n",
    "#     param_grid['selector__percentile'] = [10, 25, 50, 90, 95, 100]\n",
    "    \n",
    "    gs = GridSearchCV(pipeline,\n",
    "                      param_grid,\n",
    "                      scoring=scorer,\n",
    "                      cv=5,\n",
    "                      n_jobs=-1,\n",
    "                      verbose=1)\n",
    "    t0 = time()\n",
    "    gs.fit(X, y)\n",
    "    train_time = time() - t0\n",
    "    print(\"Train time: %0.3fs\" % train_time)\n",
    "#     print(\"Real train time: %0.3fs\" % (train_time * (TOTAL_COUNT/DEV_COUNT)))\n",
    "    print(\"Best score: %0.3f\" % gs.best_score_)\n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_params[param_name]))\n",
    "    return gs.best_score_, gs.best_estimator_\n",
    "    \n",
    "#     with open(model_name+'_best_'+type(clf).__name__+\".pkl\", 'wb') as handle:\n",
    "#         pickle.dump(best_model, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### MultinomialNB ###################################\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    7.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 9.482s\n",
      "Best score: 0.639\n",
      "\tclf__alpha: 0.001\n",
      "\tclf__fit_prior: False\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.90      0.80      0.85      6787\n",
      "       True       0.56      0.74      0.64      2341\n",
      "\n",
      "avg / total       0.81      0.78      0.79      9128\n",
      "\n",
      "################################### LinearSVC ###################################\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 75.038s\n",
      "Best score: 0.590\n",
      "\tclf__C: 0.1\n",
      "\tclf__loss: 'squared_hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.89      0.52      0.66      6787\n",
      "       True       0.37      0.81      0.51      2341\n",
      "\n",
      "avg / total       0.76      0.60      0.62      9128\n",
      "\n",
      "################################### LogisticRegression ###################################\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:   36.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 38.849s\n",
      "Best score: 0.617\n",
      "\tclf__C: 100\n",
      "\tclf__fit_intercept: False\n",
      "\tclf__penalty: 'l1'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.86      0.93      0.89      6787\n",
      "       True       0.73      0.55      0.63      2341\n",
      "\n",
      "avg / total       0.82      0.83      0.82      9128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "dict_vectorizer = DictVectorizer()\n",
    "# select_percentile = SelectPercentile(percentile=100)\n",
    "\n",
    "clf_map = [\n",
    "    (\n",
    "        MultinomialNB(),\n",
    "        {\n",
    "            'clf__alpha': [.001, .01, .1, 1],\n",
    "            'clf__fit_prior': [True, False],\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        LinearSVC(),\n",
    "        {\n",
    "            'clf__C': [.1, 1, 10, 100],\n",
    "            'clf__penalty': ['l2'],\n",
    "            'clf__loss': ['hinge', 'squared_hinge'],\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        LogisticRegression(),\n",
    "        {\n",
    "            'clf__penalty': ['l1','l2'],\n",
    "            'clf__fit_intercept': [True, False],\n",
    "            'clf__C':[.1, 1, 10, 100],\n",
    "        }\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "    #         ('selector', select_percentile),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train, clf.__class__.__name__)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_feats = [\n",
    "    'word_number_of_syllables'\n",
    "]\n",
    "\n",
    "x_data, y_data = generate_data(\"big-table-PoS.csv\", features, continuous_feats)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### MultinomialNB ###################################\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   10.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 13.246s\n",
      "Best score: 0.633\n",
      "\tclf__alpha: 0.1\n",
      "\tclf__fit_prior: False\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.90      0.80      0.84      6787\n",
      "       True       0.56      0.73      0.63      2341\n",
      "\n",
      "avg / total       0.81      0.78      0.79      9128\n",
      "\n",
      "################################### LinearSVC ###################################\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   54.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 62.130s\n",
      "Best score: 0.612\n",
      "\tclf__C: 10\n",
      "\tclf__loss: 'squared_hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.86      0.93      0.89      6787\n",
      "       True       0.72      0.55      0.62      2341\n",
      "\n",
      "avg / total       0.82      0.83      0.82      9128\n",
      "\n",
      "################################### LogisticRegression ###################################\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 84.955s\n",
      "Best score: 0.611\n",
      "\tclf__C: 10\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.86      0.92      0.89      6787\n",
      "       True       0.72      0.55      0.62      2341\n",
      "\n",
      "avg / total       0.82      0.83      0.82      9128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "    #         ('selector', select_percentile),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train, clf.__class__.__name__)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'word',\n",
    "    'word_pos_tag',\n",
    "    'word_pos_tag_simplified'\n",
    "]\n",
    "continuous_feats = []\n",
    "\n",
    "x_data, y_data = generate_data(\"big-table-PoS.csv\", features, continuous_feats)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### MultinomialNB ###################################\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    5.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 7.661s\n",
      "Best score: 0.639\n",
      "\tclf__alpha: 0.1\n",
      "\tclf__fit_prior: False\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.90      0.79      0.84      6787\n",
      "       True       0.56      0.76      0.64      2341\n",
      "\n",
      "avg / total       0.82      0.78      0.79      9128\n",
      "\n",
      "################################### LinearSVC ###################################\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   36.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 41.422s\n",
      "Best score: 0.614\n",
      "\tclf__C: 100\n",
      "\tclf__loss: 'squared_hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.86      0.91      0.88      6787\n",
      "       True       0.69      0.57      0.62      2341\n",
      "\n",
      "avg / total       0.82      0.82      0.82      9128\n",
      "\n",
      "################################### LogisticRegression ###################################\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:   44.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 47.581s\n",
      "Best score: 0.615\n",
      "\tclf__C: 100\n",
      "\tclf__fit_intercept: False\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.86      0.93      0.89      6787\n",
      "       True       0.73      0.54      0.62      2341\n",
      "\n",
      "avg / total       0.82      0.83      0.82      9128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "    #         ('selector', select_percentile),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train, clf.__class__.__name__)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'word',\n",
    "    'word_pos_tag',\n",
    "    'word_pos_tag_simplified',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_ipu',\n",
    "]\n",
    "\n",
    "continuous_feats = [\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_ipu',\n",
    "]\n",
    "\n",
    "x_data, y_data = generate_data(\"big-table-PoS.csv\", features, continuous_feats)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### MultinomialNB ###################################\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    6.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 8.829s\n",
      "Best score: 0.634\n",
      "\tclf__alpha: 0.01\n",
      "\tclf__fit_prior: False\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.90      0.80      0.84      6787\n",
      "       True       0.56      0.74      0.64      2341\n",
      "\n",
      "avg / total       0.81      0.78      0.79      9128\n",
      "\n",
      "################################### LinearSVC ###################################\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 70.579s\n",
      "Best score: 0.614\n",
      "\tclf__C: 1\n",
      "\tclf__loss: 'squared_hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.86      0.93      0.89      6787\n",
      "       True       0.73      0.55      0.62      2341\n",
      "\n",
      "avg / total       0.82      0.83      0.82      9128\n",
      "\n",
      "################################### LogisticRegression ###################################\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:   46.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 55.538s\n",
      "Best score: 0.617\n",
      "\tclf__C: 100\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l1'\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.86      0.93      0.89      6787\n",
      "       True       0.73      0.55      0.62      2341\n",
      "\n",
      "avg / total       0.82      0.83      0.82      9128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "    #         ('selector', select_percentile),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train, clf.__class__.__name__)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'word',\n",
    "    'word_pos_tag',\n",
    "    'word_pos_tag_simplified',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_ipu',\n",
    "]\n",
    "\n",
    "continuous_feats = [\n",
    "    'word_number_of_syllables',\n",
    "]\n",
    "\n",
    "x_data, y_data = generate_data(\"big-table-PoS.csv\", features, continuous_feats)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### MultinomialNB ###################################\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 8.388s\n",
      "Best score: 0.634\n",
      "\tclf__alpha: 0.01\n",
      "\tclf__fit_prior: False\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.90      0.80      0.85      6787\n",
      "       True       0.56      0.74      0.63      2341\n",
      "\n",
      "avg / total       0.81      0.78      0.79      9128\n",
      "\n",
      "################################### LinearSVC ###################################\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    }
   ],
   "source": [
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "    #         ('selector', select_percentile),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train, clf.__class__.__name__)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'word',\n",
    "    'word_pos_tag',\n",
    "    'word_pos_tag_simplified',\n",
    "    'word_number_of_syllables',\n",
    "]\n",
    "\n",
    "continuous_feats = [\n",
    "    'word_number_of_syllables',\n",
    "]\n",
    "\n",
    "x_data, y_data = generate_data(\"big-table-PoS.csv\", features, continuous_feats)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2557)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (clf, param_grid) in clf_map:\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "    #         ('selector', select_percentile),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    best_score, best_model = classify_my_model(pipeline, param_grid, X_train, y_train, clf.__class__.__name__)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding POS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best one was LogReg with only num of syllables as a continuous feature\n",
    "features = [\n",
    "    'word',\n",
    "    'word_pos_tag',\n",
    "    'word_pos_tag_simplified',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_turn',\n",
    "    'word_number_in_task',\n",
    "    'total_number_of_words_in_turn',\n",
    "    'total_number_of_words_in_task'\n",
    "]\n",
    "feat_indices = [18, 19, 20, 22, 11, 12, 14, 15]\n",
    "label_index = 27\n",
    "continuous_feats = [\n",
    "    'word_number_of_syllables'\n",
    "]\n",
    "## Read the file\n",
    "file_name = \"big-table-PoS.csv\"\n",
    "x_data = []\n",
    "y_data = []\n",
    "labels = []\n",
    "with open(file_name, 'r') as f:\n",
    "    for i, l in enumerate(csv.reader(f)):\n",
    "        if i == 0: continue\n",
    "#         elif i == 2: print x_data, y_data\n",
    "        feats = {feat: l[i] for feat, i in zip(features,feat_indices)}\n",
    "        # convert some to continuous features\n",
    "        for feat in continuous_feats:\n",
    "            feats[feat] = float(feats[feat])\n",
    "        x_data.append(feats)\n",
    "        label = l[label_index] == \"4\" or l[label_index] == \"4-\" or l[label_index] == \"4p\"\n",
    "        y_data.append(label)\n",
    "        labels.append(l[label_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance from end of ipu and turn\n",
    "DIST_END_TURN = \"DIST_END_TURN\"\n",
    "for i in range(len(x_data)):\n",
    "    x_data[i][DIST_END_TURN] = int(x_data[i]['total_number_of_words_in_turn']) - int(x_data[i]['word_number_in_turn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Bigram, segmented by IPU (don't use this, since we're not given IPU information from text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_IPU_BIGRAM_LEFT = \"POS_IPU_BIGRAM_LEFT\"\n",
    "POS_IPU_BIGRAM_RIGHT = \"POS_IPU_BIGRAM_RIGHT\"\n",
    "for i in range(len(x_data)):\n",
    "    # to the left\n",
    "    if x_data[i]['word_number_in_ipu'] == '1':\n",
    "        x_data[i][POS_IPU_BIGRAM_LEFT] = 'BEGIN/'+x_data[i]['word_pos_tag']\n",
    "    else:\n",
    "        x_data[i][POS_IPU_BIGRAM_LEFT] = x_data[i-1]['word_pos_tag']+\"/\"+x_data[i]['word_pos_tag']\n",
    "    # to the right\n",
    "    if x_data[i]['word_number_in_ipu'] == x_data[i]['total_number_of_words_in_ipu']:\n",
    "        x_data[i][POS_IPU_BIGRAM_RIGHT] = x_data[i]['word_pos_tag']+\"/END\"\n",
    "    else:\n",
    "        x_data[i][POS_IPU_BIGRAM_RIGHT] = x_data[i]['word_pos_tag']+\"/\"+x_data[i+1]['word_pos_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With dist features\n",
    "for (clf, param_grid) in clf_map:\n",
    "    classify_my_model(clf, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_data)):\n",
    "    del x_data[i][POS_IPU_BIGRAM_LEFT]\n",
    "    del x_data[i][POS_IPU_BIGRAM_RIGHT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Bigram, segmented by TURN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using both turn and ipu pos bigrams\n",
    "POS_TURN_BIGRAM_LEFT = \"POS_TURN_BIGRAM_LEFT\"\n",
    "POS_TURN_BIGRAM_RIGHT = \"POS_TURN_BIGRAM_RIGHT\"\n",
    "for i in range(len(x_data)):\n",
    "    # to the left\n",
    "    if x_data[i]['word_number_in_turn'] == '1':\n",
    "        x_data[i][POS_TURN_BIGRAM_LEFT] = 'BEGIN/'+x_data[i]['word_pos_tag']\n",
    "    else:\n",
    "        x_data[i][POS_TURN_BIGRAM_LEFT] = x_data[i-1]['word_pos_tag']+\"/\"+x_data[i]['word_pos_tag']\n",
    "    # to the right\n",
    "    if x_data[i]['word_number_in_turn'] == x_data[i]['total_number_of_words_in_turn']:\n",
    "        x_data[i][POS_TURN_BIGRAM_RIGHT] = x_data[i]['word_pos_tag']+\"/END\"\n",
    "    else:\n",
    "        x_data[i][POS_TURN_BIGRAM_RIGHT] = x_data[i]['word_pos_tag']+\"/\"+x_data[i+1]['word_pos_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (clf, param_grid) in clf_map:\n",
    "    classify_my_model(clf, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_data)):\n",
    "    del x_data[i][POS_TURN_BIGRAM_LEFT]\n",
    "    del x_data[i][POS_TURN_BIGRAM_RIGHT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmented by IPU is better than turn. Let's add the feature of whether word ends with a hyphen (denotes a stutter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_STUTTER = \"IS_STUTTER\"\n",
    "for i in range(len(x_data)):\n",
    "    x_data[i][IS_STUTTER] = x_data[i]['word'][-1] == '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (clf, param_grid) in clf_map:\n",
    "    classify_my_model(clf, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmented by TURN trigram as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_IPU_TRIGRAM = \"POS_TURN_TRIGRAM\"\n",
    "for i in range(len(x_data)):\n",
    "    # to the left\n",
    "    left = \"BEGIN\"\n",
    "    if x_data[i]['word_number_in_turn'] != '1':\n",
    "        left = x_data[i-1]['word_pos_tag']\n",
    "    # to the right\n",
    "    right = \"END\"\n",
    "    if x_data[i]['word_number_in_turn'] != x_data[i]['total_number_of_words_in_turn']:\n",
    "        right = x_data[i+1]['word_pos_tag']\n",
    "    x_data[i][POS_IPU_TRIGRAM] = left+\"/\"+x_data[i]['word']+\"/\"+right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (clf, param_grid) in clf_map:\n",
    "    classify_my_model(clf, param_grid, 'turn_trigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Adding syntactic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r\"'\").match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_SUBTREE = \"SIZE_SUBTREE\"\n",
    "NUM_SIBLINGS = \"NUM_SIBLINGS\"\n",
    "# Because we can't assume that every turn contains a fully connected dependency parse, we can treat distance as either 1, 2, or 3 (meaning 2 or more)\n",
    "# dist = 0 when this term is last term\n",
    "# dist = 1 when next term is head or a child of current term\n",
    "# else dist = 2 when there is overlap between (current term's head and children) and (next term's head and children)\n",
    "# else dist = 3\n",
    "ARC_DIST_NEXT = \"ARC_DIST_NEXT\"\n",
    "FUNC = \"FUNC\"\n",
    "def subtree_size(token):\n",
    "    return sum([subtree_size(child) for child in token.children])\n",
    "def num_siblings(token):\n",
    "    return len(list(token.head.children))\n",
    "def arc_dist(token, next_token):\n",
    "    if not next_token: return 0\n",
    "    elif next_token.text == token.head.text or next_token.text in [a.text for a in token.children]: return 1\n",
    "    elif any([a.text in [b.text for b in list(token.children)+[token.head]] for a in list(next_token.children)+[next_token.head]]): return 2\n",
    "    else: return 3\n",
    "def get_func(token):\n",
    "    if \"SUBJ\" in token.dep_: return 0\n",
    "    elif \"DOBJ\" in token.dep_: return 1\n",
    "    elif \"POBJ\" in token.dep_: return 2\n",
    "    else: return 3\n",
    "# set these features to 0 by default\n",
    "for i in range(len(x_data)):\n",
    "    x_data[i][SIZE_SUBTREE] = '-1'\n",
    "    x_data[i][NUM_SIBLINGS] = '-1'\n",
    "    x_data[i][ARC_DIST_NEXT] = '-1'\n",
    "    x_data[i][FUNC] = '-1'\n",
    "for i in range(len(x_data)):\n",
    "    if x_data[i]['word_number_in_turn'] == '1':\n",
    "        # figure out the current turn\n",
    "        turn = ' '.join([word['word'] for word in x_data[i:i+int(x_data[i]['total_number_of_words_in_turn'])]])\n",
    "#         print(turn)\n",
    "        doc = nlp(turn)\n",
    "        for j, token in enumerate(doc):\n",
    "#             print(token)\n",
    "            next_token = doc[i+1] if i+1<len(doc) else None\n",
    "            x_data[i+j][SIZE_SUBTREE] = str(subtree_size(token))\n",
    "            x_data[i+j][NUM_SIBLINGS] = str(num_siblings(token))\n",
    "            x_data[i+j][ARC_DIST_NEXT] = str(arc_dist(token, next_token))\n",
    "            x_data[i+j][FUNC] = str(get_func(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (clf, param_grid) in clf_map:\n",
    "    classify_my_model(clf, param_grid, 'syntactic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most significant features\n",
    "model = pickle.load(open('syntactic_best_LogisticRegression.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n",
    "def show_least_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names), key=lambda x: abs(x[0]))\n",
    "    top = coefs_with_fns[:n]\n",
    "    for (coef_1, fn_1) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\" % (coef_1, fn_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_most_informative_features(model.steps[0][1], model.steps[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_least_informative_features(model.steps[0][1], model.steps[2][1], n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.steps[2][1].coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def show_feature_importance(vectorizer, clf):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = zip(clf.coef_[0], feature_names)\n",
    "    feat_count = defaultdict(list)\n",
    "    for (coef_1, fn_1) in coefs_with_fns:\n",
    "        feat = fn_1.split('=')[0]\n",
    "        feat_count[feat].append(abs(coef_1))\n",
    "    for feat in feat_count.keys():\n",
    "        print(feat, sum(feat_count[feat])/len(feat_count[feat]))\n",
    "    return feat_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_count = show_feature_importance(model.steps[0][1], model.steps[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for feat in feat_count.keys():\n",
    "    hist, bins = np.histogram(feat_count[feat], bins=int(len(feat_count[feat])**0.5))\n",
    "    width = 0.7 * (bins[1] - bins[0])\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.bar(center, hist, align='center', width=width)\n",
    "    plt.title(feat)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
