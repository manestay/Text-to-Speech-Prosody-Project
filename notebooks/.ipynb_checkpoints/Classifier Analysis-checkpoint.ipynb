{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "from time import time\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "best_model = None\n",
    "\n",
    "dict_vectorizer = DictVectorizer()\n",
    "select_percentile = SelectPercentile(percentile=100)\n",
    "\n",
    "def classify_my_model(clf, param_grid):\n",
    "    global best_score, best_model\n",
    "    print('###################################',type(clf),'#########################################')\n",
    "    folds = StratifiedKFold(y_train, n_folds=3, shuffle=True, random_state=int(time()))\n",
    "    pipeline = Pipeline([\n",
    "        ('dictvec', dict_vectorizer),\n",
    "        ('selector', select_percentile),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "    gs = GridSearchCV(pipeline,\n",
    "                      param_grid,\n",
    "                      scoring='f1',\n",
    "                      cv=folds,\n",
    "                      n_jobs=-1,\n",
    "                      verbose=1)\n",
    "    t0 = time()\n",
    "    gs.fit(x_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"Train time: %0.3fs\" % train_time)\n",
    "    print(\"Best score: %0.3f\" % gs.best_score_)\n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_params[param_name]))\n",
    "    best_score = gs.best_score_\n",
    "    best_model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best one was LogReg with only num of syllables as a continuous feature\n",
    "features = [\n",
    "    'word',\n",
    "    'word_pos_tag',\n",
    "    'word_pos_tag_simplified',\n",
    "    'word_number_of_syllables',\n",
    "    'word_number_in_ipu',\n",
    "    'word_number_in_turn',\n",
    "    'word_number_in_task',\n",
    "    'total_number_of_words_in_ipu',\n",
    "    'total_number_of_words_in_turn',\n",
    "    'total_number_of_words_in_task'\n",
    "]\n",
    "feat_indices = [18, 19, 20, 22, 10, 11, 12, 13, 14, 15]\n",
    "label_index = 27\n",
    "continuous_feats = [\n",
    "    'word_number_of_syllables'\n",
    "]\n",
    "## Read the file\n",
    "file_name = \"big-table-PoS.csv\"\n",
    "x_data = []\n",
    "y_data = []\n",
    "labels = []\n",
    "with open(file_name, 'r') as f:\n",
    "    for i, l in enumerate(csv.reader(f)):\n",
    "        if i == 0: continue\n",
    "#         elif i == 2: print x_data, y_data\n",
    "        feats = {feat: l[i] for feat, i in zip(features,feat_indices)}\n",
    "        # convert some to continuous features\n",
    "        for feat in continuous_feats:\n",
    "            feats[feat] = float(feats[feat])\n",
    "        x_data.append(feats)\n",
    "        label = l[label_index] == \"4\" or l[label_index] == \"4-\" or l[label_index] == \"4p\"\n",
    "        y_data.append(label)\n",
    "        labels.append(l[label_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data = list(zip(x_data, y_data))\n",
    "random.shuffle(data)\n",
    "test_data = data[:4000]\n",
    "train_data = data[4000:]\n",
    "x_train, y_train = zip(*train_data)\n",
    "x_test, y_test = zip(*test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### <class 'sklearn.linear_model.logistic.LogisticRegression'> #########################################\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 231.285s\n",
      "Best score: 0.758\n",
      "\tclf__C: 1\n",
      "\tclf__fit_intercept: False\n",
      "\tclf__penalty: 'l1'\n",
      "\tselector__percentile: 100\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'clf__penalty': ['l1','l2'],\n",
    "    'clf__fit_intercept': [True, False],\n",
    "    'clf__C':[1, 10, 100],\n",
    "    'selector__percentile': [90, 95, 100]\n",
    "}\n",
    "clf = LogisticRegression()\n",
    "classify_my_model(clf, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load best_model from best_LogisticRegression.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_LogisticRegression.pkl', 'rb') as handle:\n",
    "    best_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze `best_model` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze false positives\n",
    "fp_terms = []\n",
    "# Analyze false negatives\n",
    "fn_terms = []\n",
    "for i in range(len(x_test)):\n",
    "    res = best_model.predict_proba(x_test[i])\n",
    "    res = res[0][1] > res[0][0]\n",
    "    if res and not y_test[i]:\n",
    "        fp_terms.append(x_test[i]) \n",
    "    elif not res and y_data[i]:\n",
    "        fn_terms.append(x_test[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate :: 0.0035\n",
      "False negative rate :: 0.23225\n"
     ]
    }
   ],
   "source": [
    "print(\"False positive rate ::\", len(fp_terms)/len(x_test))\n",
    "print(\"False negative rate ::\", len(fn_terms)/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_number_of_words_in_ipu': '3',\n",
       " 'total_number_of_words_in_task': '12',\n",
       " 'total_number_of_words_in_turn': '3',\n",
       " 'word': 'okay',\n",
       " 'word_number_in_ipu': '1',\n",
       " 'word_number_in_task': '4',\n",
       " 'word_number_in_turn': '1',\n",
       " 'word_number_of_syllables': 2.0,\n",
       " 'word_pos_tag': 'UH',\n",
       " 'word_pos_tag_simplified': 'O'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_terms[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's do a reduce over word to see if some patterns arise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_counts = defaultdict(int)\n",
    "for term in x_test:\n",
    "    word_counts[term['word']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fp_word = defaultdict(int)\n",
    "for term in fp_terms:\n",
    "    fp_word[term['word']] += 1\n",
    "most_fp_words = sorted(list(fp_word.keys()), key=lambda x: -fp_word[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay \t 7 \t 132 \t 0.05303030303030303\n",
      "that \t 6 \t 45 \t 0.13333333333333333\n",
      "uh \t 6 \t 47 \t 0.1276595744680851\n",
      "right \t 5 \t 68 \t 0.07352941176470588\n",
      "yeah \t 5 \t 56 \t 0.08928571428571429\n",
      "racket \t 4 \t 5 \t 0.8\n",
      "um \t 4 \t 54 \t 0.07407407407407407\n",
      "eye \t 4 \t 12 \t 0.3333333333333333\n",
      "mermaid \t 3 \t 16 \t 0.1875\n",
      "iron \t 3 \t 11 \t 0.2727272727272727\n",
      "lawnmower \t 3 \t 9 \t 0.3333333333333333\n",
      "middle \t 3 \t 18 \t 0.16666666666666666\n",
      "like \t 3 \t 75 \t 0.04\n",
      "no \t 3 \t 20 \t 0.15\n",
      "moon \t 3 \t 21 \t 0.14285714285714285\n",
      "mm \t 3 \t 6 \t 0.5\n",
      "lion \t 3 \t 17 \t 0.17647058823529413\n",
      "knee \t 2 \t 4 \t 0.5\n",
      "oreo \t 2 \t 3 \t 0.6666666666666666\n",
      "nail \t 2 \t 12 \t 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "for word in most_fp_words[:20]:\n",
    "    print(word, \"\\t\", fp_word[word], \"\\t\", word_counts[word], \"\\t\", fp_word[word]/word_counts[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find context of most common fp terms\n",
    "most_fp_terms = defaultdict(list)\n",
    "for term in fp_terms:\n",
    "    most_fp_terms[term['word']].append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_terms_context = []\n",
    "for i in range(len(x_data)):\n",
    "    term = x_data[i]\n",
    "    if term in most_fp_terms[term['word']]:\n",
    "        fp_terms_context.append(x_data[i-2:i+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'total_number_of_words_in_ipu': '20',\n",
       "  'total_number_of_words_in_task': '30',\n",
       "  'total_number_of_words_in_turn': '30',\n",
       "  'word': \"it's\",\n",
       "  'word_number_in_ipu': '10',\n",
       "  'word_number_in_task': '20',\n",
       "  'word_number_in_turn': '20',\n",
       "  'word_number_of_syllables': 1.0,\n",
       "  'word_pos_tag': 'PRP_BES',\n",
       "  'word_pos_tag_simplified': 'C'},\n",
       " {'total_number_of_words_in_ipu': '20',\n",
       "  'total_number_of_words_in_task': '30',\n",
       "  'total_number_of_words_in_turn': '30',\n",
       "  'word': 'a',\n",
       "  'word_number_in_ipu': '11',\n",
       "  'word_number_in_task': '21',\n",
       "  'word_number_in_turn': '21',\n",
       "  'word_number_of_syllables': 1.0,\n",
       "  'word_pos_tag': 'DT',\n",
       "  'word_pos_tag_simplified': 'O'},\n",
       " {'total_number_of_words_in_ipu': '20',\n",
       "  'total_number_of_words_in_task': '30',\n",
       "  'total_number_of_words_in_turn': '30',\n",
       "  'word': 'mirror',\n",
       "  'word_number_in_ipu': '12',\n",
       "  'word_number_in_task': '22',\n",
       "  'word_number_in_turn': '22',\n",
       "  'word_number_of_syllables': 2.0,\n",
       "  'word_pos_tag': 'NN',\n",
       "  'word_pos_tag_simplified': 'N'},\n",
       " {'total_number_of_words_in_ipu': '20',\n",
       "  'total_number_of_words_in_task': '30',\n",
       "  'total_number_of_words_in_turn': '30',\n",
       "  'word': 'or',\n",
       "  'word_number_in_ipu': '13',\n",
       "  'word_number_in_task': '23',\n",
       "  'word_number_in_turn': '23',\n",
       "  'word_number_of_syllables': 1.0,\n",
       "  'word_pos_tag': 'CC',\n",
       "  'word_pos_tag_simplified': 'O'},\n",
       " {'total_number_of_words_in_ipu': '20',\n",
       "  'total_number_of_words_in_task': '30',\n",
       "  'total_number_of_words_in_turn': '30',\n",
       "  'word': 'a',\n",
       "  'word_number_in_ipu': '14',\n",
       "  'word_number_in_task': '24',\n",
       "  'word_number_in_turn': '24',\n",
       "  'word_number_of_syllables': 1.0,\n",
       "  'word_pos_tag': 'DT',\n",
       "  'word_pos_tag_simplified': 'O'}]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_terms_context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_same_ipu = [term for term in fp_terms if term['total_number_of_words_in_ipu'] == term['word_number_in_ipu']]\n",
    "fp_same_task = [term for term in fp_terms if term['total_number_of_words_in_task'] == term['word_number_in_task']]\n",
    "fp_same_turn = [term for term in fp_terms if term['total_number_of_words_in_turn'] == term['word_number_in_turn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2389937106918239\n",
      "0.0\n",
      "0.07547169811320754\n"
     ]
    }
   ],
   "source": [
    "print(len(fp_same_ipu)/len(fp_terms))\n",
    "print(len(fp_same_task)/len(fp_terms))\n",
    "print(len(fp_same_turn)/len(fp_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1320754716981132\n"
     ]
    }
   ],
   "source": [
    "fp_single_utterance = [term for term in fp_terms if term['total_number_of_words_in_ipu'] == term['word_number_in_ipu'] and term['word_number_in_ipu'] == '1']\n",
    "print(len(fp_single_utterance)/len(fp_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_word = defaultdict(int)\n",
    "for term in fn_terms:\n",
    "    fn_word[term['word']] += 1\n",
    "most_fn_words = sorted(list(fn_word.keys()), key=lambda x: -fn_word[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the \t 116 \t 477 \t 0.2431865828092243\n",
      "of \t 33 \t 127 \t 0.25984251968503935\n",
      "a \t 29 \t 98 \t 0.29591836734693877\n",
      "and \t 24 \t 126 \t 0.19047619047619047\n",
      "I \t 23 \t 78 \t 0.2948717948717949\n",
      "on \t 16 \t 68 \t 0.23529411764705882\n",
      "so \t 15 \t 46 \t 0.32608695652173914\n",
      "like \t 15 \t 68 \t 0.22058823529411764\n",
      "that \t 15 \t 66 \t 0.22727272727272727\n",
      "it's \t 15 \t 63 \t 0.23809523809523808\n",
      "you \t 13 \t 40 \t 0.325\n",
      "is \t 12 \t 68 \t 0.17647058823529413\n",
      "see \t 11 \t 29 \t 0.3793103448275862\n",
      "right \t 10 \t 68 \t 0.14705882352941177\n",
      "with \t 10 \t 29 \t 0.3448275862068966\n",
      "top \t 9 \t 34 \t 0.2647058823529412\n",
      "blue \t 8 \t 28 \t 0.2857142857142857\n",
      "little \t 7 \t 21 \t 0.3333333333333333\n",
      "uh \t 7 \t 44 \t 0.1590909090909091\n",
      "it \t 7 \t 55 \t 0.12727272727272726\n"
     ]
    }
   ],
   "source": [
    "for word in most_fn_words[:20]:\n",
    "    print(word, \"\\t\", fn_word[word], \"\\t\", word_counts[word], \"\\t\", fn_word[word]/word_counts[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find context of most common fp terms\n",
    "most_fn_terms = defaultdict(list)\n",
    "for term in fn_terms:\n",
    "    most_fn_terms[term['word']].append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_terms_context = []\n",
    "for i in range(len(x_data)):\n",
    "    term = x_data[i]\n",
    "    if term in most_fn_terms[term['word']]:\n",
    "        fn_terms_context.append(x_data[i-2:i+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'total_number_of_words_in_ipu': '5',\n",
       "  'total_number_of_words_in_task': '5',\n",
       "  'total_number_of_words_in_turn': '5',\n",
       "  'word': 'I',\n",
       "  'word_number_in_ipu': '2',\n",
       "  'word_number_in_task': '2',\n",
       "  'word_number_in_turn': '2',\n",
       "  'word_number_of_syllables': 1.0,\n",
       "  'word_pos_tag': 'PRP',\n",
       "  'word_pos_tag_simplified': 'O'},\n",
       " {'total_number_of_words_in_ipu': '26',\n",
       "  'total_number_of_words_in_task': '26',\n",
       "  'total_number_of_words_in_turn': '26',\n",
       "  'word': 'wine',\n",
       "  'word_number_in_ipu': '20',\n",
       "  'word_number_in_task': '20',\n",
       "  'word_number_in_turn': '20',\n",
       "  'word_number_of_syllables': 1.0,\n",
       "  'word_pos_tag': 'NN',\n",
       "  'word_pos_tag_simplified': 'N'},\n",
       " {'total_number_of_words_in_ipu': '2',\n",
       "  'total_number_of_words_in_task': '2',\n",
       "  'total_number_of_words_in_turn': '2',\n",
       "  'word': 'got',\n",
       "  'word_number_in_ipu': '1',\n",
       "  'word_number_in_task': '1',\n",
       "  'word_number_in_turn': '1',\n",
       "  'word_number_of_syllables': 1.0,\n",
       "  'word_pos_tag': 'VBD',\n",
       "  'word_pos_tag_simplified': 'V'},\n",
       " {'total_number_of_words_in_ipu': '23',\n",
       "  'total_number_of_words_in_task': '23',\n",
       "  'total_number_of_words_in_turn': '23',\n",
       "  'word': 'we',\n",
       "  'word_number_in_ipu': '9',\n",
       "  'word_number_in_task': '9',\n",
       "  'word_number_in_turn': '9',\n",
       "  'word_number_of_syllables': 1.0,\n",
       "  'word_pos_tag': 'PRP',\n",
       "  'word_pos_tag_simplified': 'O'},\n",
       " {'total_number_of_words_in_ipu': '2',\n",
       "  'total_number_of_words_in_task': '50',\n",
       "  'total_number_of_words_in_turn': '27',\n",
       "  'word': 'looking',\n",
       "  'word_number_in_ipu': '2',\n",
       "  'word_number_in_task': '4',\n",
       "  'word_number_in_turn': '3',\n",
       "  'word_number_of_syllables': 2.0,\n",
       "  'word_pos_tag': 'VBG',\n",
       "  'word_pos_tag_simplified': 'V'},\n",
       " {'total_number_of_words_in_ipu': '22',\n",
       "  'total_number_of_words_in_task': '22',\n",
       "  'total_number_of_words_in_turn': '22',\n",
       "  'word': 'up',\n",
       "  'word_number_in_ipu': '15',\n",
       "  'word_number_in_task': '15',\n",
       "  'word_number_in_turn': '15',\n",
       "  'word_number_of_syllables': 1.0,\n",
       "  'word_pos_tag': 'IN',\n",
       "  'word_pos_tag_simplified': 'O'},\n",
       " {'total_number_of_words_in_ipu': '2',\n",
       "  'total_number_of_words_in_task': '2',\n",
       "  'total_number_of_words_in_turn': '2',\n",
       "  'word': 'got',\n",
       "  'word_number_in_ipu': '1',\n",
       "  'word_number_in_task': '1',\n",
       "  'word_number_in_turn': '1',\n",
       "  'word_number_of_syllables': 1.0,\n",
       "  'word_pos_tag': 'VBD',\n",
       "  'word_pos_tag_simplified': 'V'},\n",
       " {'total_number_of_words_in_ipu': '10',\n",
       "  'total_number_of_words_in_task': '26',\n",
       "  'total_number_of_words_in_turn': '26',\n",
       "  'word': 'and',\n",
       "  'word_number_in_ipu': '10',\n",
       "  'word_number_in_task': '10',\n",
       "  'word_number_in_turn': '10',\n",
       "  'word_number_of_syllables': 1.0,\n",
       "  'word_pos_tag': 'CC',\n",
       "  'word_pos_tag_simplified': 'O'},\n",
       " {'total_number_of_words_in_ipu': '6',\n",
       "  'total_number_of_words_in_task': '18',\n",
       "  'total_number_of_words_in_turn': '18',\n",
       "  'word': 'on',\n",
       "  'word_number_in_ipu': '5',\n",
       "  'word_number_in_task': '5',\n",
       "  'word_number_in_turn': '5',\n",
       "  'word_number_of_syllables': 1.0,\n",
       "  'word_pos_tag': 'IN',\n",
       "  'word_pos_tag_simplified': 'O'}]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a[2] for a in fn_terms_context[1:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_same_ipu = [term for term in fn_terms if term['total_number_of_words_in_ipu'] == term['word_number_in_ipu']]\n",
    "fn_same_task = [term for term in fn_terms if term['total_number_of_words_in_task'] == term['word_number_in_task']]\n",
    "fn_same_turn = [term for term in fn_terms if term['total_number_of_words_in_turn'] == term['word_number_in_turn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07593307593307594\n",
      "0.003861003861003861\n",
      "0.023166023166023165\n"
     ]
    }
   ],
   "source": [
    "print(len(fn_same_ipu)/len(fn_terms))\n",
    "print(len(fn_same_task)/len(fn_terms))\n",
    "print(len(fn_same_turn)/len(fn_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007722007722007722\n"
     ]
    }
   ],
   "source": [
    "fn_single_utterance = [term for term in fn_terms if term['total_number_of_words_in_ipu'] == term['word_number_in_ipu'] and term['word_number_in_ipu'] == '1']\n",
    "print(len(fn_single_utterance)/len(fn_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## then pos tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts = defaultdict(int)\n",
    "for term in x_test:\n",
    "    pos_counts[term['word_pos_tag']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_pos = defaultdict(int)\n",
    "for term in fp_terms:\n",
    "    fp_pos[term['word_pos_tag']] += 1\n",
    "most_fp_pos = sorted(list(fp_pos.keys()), key=lambda x: -fp_pos[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN \t 67 \t 628 \t 0.10668789808917198\n",
      "UH \t 37 \t 417 \t 0.08872901678657075\n",
      "RB \t 14 \t 308 \t 0.045454545454545456\n",
      "NNS \t 8 \t 55 \t 0.14545454545454545\n",
      "JJ \t 5 \t 266 \t 0.018796992481203006\n",
      "VBZ \t 5 \t 106 \t 0.04716981132075472\n",
      "VBN \t 3 \t 29 \t 0.10344827586206896\n",
      "RP \t 3 \t 18 \t 0.16666666666666666\n",
      "CC \t 2 \t 143 \t 0.013986013986013986\n",
      "NNP \t 2 \t 19 \t 0.10526315789473684\n",
      "PRP \t 2 \t 189 \t 0.010582010582010581\n",
      "IN \t 2 \t 462 \t 0.004329004329004329\n",
      "DT \t 2 \t 678 \t 0.0029498525073746312\n",
      "VBP \t 2 \t 79 \t 0.02531645569620253\n",
      "WDT \t 1 \t 11 \t 0.09090909090909091\n",
      "VBD \t 1 \t 41 \t 0.024390243902439025\n",
      "VB \t 1 \t 95 \t 0.010526315789473684\n",
      "PRP_BES \t 1 \t 72 \t 0.013888888888888888\n",
      "CD \t 1 \t 42 \t 0.023809523809523808\n"
     ]
    }
   ],
   "source": [
    "for pos in most_fp_pos[:20]:\n",
    "    print(pos, \"\\t\", fp_pos[pos], \"\\t\", pos_counts[pos], \"\\t\", fp_pos[pos]/pos_counts[pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_pos = defaultdict(int)\n",
    "for term in fn_terms:\n",
    "    fn_pos[term['word_pos_tag']] += 1\n",
    "most_fn_pos = sorted(list(fn_pos.keys()), key=lambda x: -fn_pos[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT \t 168 \t 678 \t 0.24778761061946902\n",
      "IN \t 106 \t 462 \t 0.22943722943722944\n",
      "NN \t 83 \t 628 \t 0.1321656050955414\n",
      "JJ \t 61 \t 266 \t 0.22932330827067668\n",
      "RB \t 57 \t 308 \t 0.18506493506493507\n",
      "PRP \t 47 \t 189 \t 0.24867724867724866\n",
      "CC \t 30 \t 143 \t 0.2097902097902098\n",
      "VB \t 29 \t 95 \t 0.30526315789473685\n",
      "VBZ \t 24 \t 106 \t 0.22641509433962265\n",
      "UH \t 24 \t 417 \t 0.05755395683453238\n",
      "VBP \t 22 \t 79 \t 0.27848101265822783\n",
      "PRP_BES \t 17 \t 72 \t 0.2361111111111111\n",
      "VBG \t 14 \t 50 \t 0.28\n",
      "XX \t 9 \t 46 \t 0.1956521739130435\n",
      "VBD \t 9 \t 41 \t 0.21951219512195122\n",
      "CD \t 8 \t 42 \t 0.19047619047619047\n",
      "MD \t 8 \t 35 \t 0.22857142857142856\n",
      "NNS \t 8 \t 55 \t 0.14545454545454545\n",
      "PRP$ \t 8 \t 27 \t 0.2962962962962963\n",
      "PRP_VBP \t 5 \t 33 \t 0.15151515151515152\n"
     ]
    }
   ],
   "source": [
    "for pos in most_fn_pos[:20]:\n",
    "    print(pos, \"\\t\", fn_pos[pos], \"\\t\", pos_counts[pos], \"\\t\", fn_pos[pos]/pos_counts[pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## then number of syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "nos_counts = defaultdict(int)\n",
    "for term in x_test:\n",
    "    nos_counts[term['word_number_of_syllables']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_nos = defaultdict(int)\n",
    "for term in fp_terms:\n",
    "    fp_nos[term['word_number_of_syllables']] += 1\n",
    "most_fp_nos = sorted(list(fp_nos.keys()), key=lambda x: -fp_nos[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 \t 94 \t 3147 \t 0.02986971719097553\n",
      "2.0 \t 55 \t 746 \t 0.07372654155495978\n",
      "3.0 \t 6 \t 79 \t 0.0759493670886076\n",
      "4.0 \t 2 \t 22 \t 0.09090909090909091\n",
      "5.0 \t 2 \t 6 \t 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "for nos in most_fp_nos[:20]:\n",
    "    print(nos, \"\\t\", fp_nos[nos], \"\\t\", nos_counts[nos], \"\\t\", fp_nos[nos]/nos_counts[nos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_nos = defaultdict(int)\n",
    "for term in fn_terms:\n",
    "    fn_nos[term['word_number_of_syllables']] += 1\n",
    "most_fn_nos = sorted(list(fn_nos.keys()), key=lambda x: -fn_nos[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 \t 649 \t 3147 \t 0.20622815379726725\n",
      "2.0 \t 114 \t 746 \t 0.15281501340482573\n",
      "3.0 \t 9 \t 79 \t 0.11392405063291139\n",
      "4.0 \t 4 \t 22 \t 0.18181818181818182\n",
      "5.0 \t 1 \t 6 \t 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "for nos in most_fn_nos[:20]:\n",
    "    print(nos, \"\\t\", fn_nos[nos], \"\\t\", nos_counts[nos], \"\\t\", fn_nos[nos]/nos_counts[nos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we even find single-word IPUs or turns with no level 4 phrase boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[391, 1692, 1823, 2034, 2035, 2122, 2233, 2427, 2671, 2680, 2688, 2842, 2887, 2888, 2906, 2934, 2965, 2981, 2982, 3067, 3119, 3205, 3245, 3314, 3367, 3408, 3540, 3632, 3633, 3650, 3733, 3761, 3768, 3812, 3840, 3957, 3969, 4015, 4043, 4165, 4317, 4360, 4380, 4812, 4922, 4927, 5035, 5101, 5149, 5241, 5263, 5277, 5437, 5695, 5761, 6228, 6483, 6638, 6961, 7780, 7859, 8263, 8483, 8525, 8526, 8527, 8563, 8605, 8635, 8703, 8715, 8831, 8845, 9010, 9088, 9145, 9401, 9420, 9461, 9462, 9463, 9468, 9513, 9545, 9549, 9599, 9613, 9936, 10221, 10312, 10457, 10511, 10572, 11451, 11452, 11527, 11550, 11558, 11620, 11645, 11696, 11711, 11756, 12002, 12010, 12140, 12203, 12278, 12299, 12376, 12433, 12509, 12523, 12565, 12629, 12655, 12667, 12922, 13987, 14190, 14294, 14438, 14452, 14457, 14936, 15295, 15311, 15436, 15516, 15517, 15519, 15727, 15761, 15766, 15781, 15789, 15833, 15835, 15997, 16089, 16243, 16299, 16453, 16595, 16605, 16628, 16758, 16984, 16985, 17002, 17026, 17078, 18923, 18957, 19075, 19115, 19352, 19353, 19441, 19468, 19568, 19641, 19691, 19834, 19841, 19856, 19940, 20027, 20137, 20199, 20202, 20212, 20222, 20232, 20321, 20424, 20446, 20447, 20464, 20465, 20477, 20582, 20614, 20615, 20949, 21551, 21582, 21589, 21621, 21727, 21735, 21826, 21848, 22555, 22582, 22613, 22614, 22654, 23293, 23452, 23490, 23491, 23493, 23522, 23603, 23760, 23971, 23989, 24120, 24262, 24282, 24283, 24284, 24322, 24380, 24389, 24433, 24462, 24486, 24515, 24581, 24720, 24726, 24727, 24740, 24928, 25038, 25166, 25281, 25328, 25412, 25524, 25599, 25910, 25962, 26177, 26197, 26296, 26321, 26322, 26327, 26349, 26356, 26369, 26380, 26381, 26384, 26385, 26408, 26451, 26524, 26529, 26551, 26552, 26574, 26610, 26630, 26635, 26658, 26659, 26687, 26693, 26721, 26722, 26732, 26806, 26812, 26813, 26898, 26923, 26935, 26936, 26937, 26946, 26947, 26959, 27075, 27143, 27144, 27195, 27200, 27201, 27261, 27271, 27318, 27342, 27352, 27367, 27424, 27442, 27443, 27542, 27569, 27591, 27609, 27610, 27688, 27883, 27927, 27928, 28090, 28097, 28150, 28160, 28173, 28207, 28211, 28212, 28309, 28364, 28498, 28602, 28684, 28766, 28825, 28827, 28874, 28926, 29096, 29444, 29756, 29822, 29956, 29974, 30056, 30206, 30252, 30292, 30324, 30453, 30613, 30658, 30699, 30703, 30773, 30782, 31109, 31530, 31535, 31971, 31975, 32154, 32208, 32299, 32356, 32358, 32391, 32500, 32515, 32568, 32714, 33432, 33939, 33968, 34107, 34229, 34240, 34375, 34376, 34430, 34458, 34489, 34490, 34617, 34671, 34791, 34921, 35235, 35353, 35777, 35790, 35880, 35901, 35917, 36209, 36277, 36384, 36442, 36811, 36848, 36987, 37138, 37222, 37379, 37410, 37432, 37523, 37621, 37693, 37809, 37903, 38294, 38492, 38585, 38587, 38661, 38777, 38795, 38814, 38823, 38853, 39015, 39161, 39497, 39543, 39846, 40113, 40128, 40129, 40160, 40166, 40195, 40418, 40566, 40567, 40568, 40749, 40825, 40902, 40903, 40926, 40953, 41033, 41036, 41089, 41325, 41338, 41351, 41577, 41611, 41683, 41750, 41800, 41817, 41841, 41872, 41878, 41904, 41907, 41911, 41951, 41952, 42012, 42096, 42145, 42169, 42178, 42209, 42235, 42254, 42266, 42276, 42304, 42325, 42455, 42613, 42649, 42665, 42686, 42693, 43270, 43508, 43577, 43605, 43725, 43793, 43967, 44051, 44197, 44203, 44412, 44436, 44502, 44517, 44862, 44953, 45282, 45343]\n",
      "[2427, 2688, 2887, 2906, 2965, 3245, 3768, 3812, 5101, 5149, 6228, 7859, 9088, 9461, 9462, 9545, 10221, 11527, 11696, 11711, 11756, 12278, 12433, 12523, 12655, 13987, 14457, 15295, 15781, 15997, 16243, 19075, 20199, 20202, 20424, 22555, 23452, 23760, 23971, 23989, 24322, 24380, 24581, 24928, 25166, 25328, 25524, 26369, 26408, 26524, 26551, 26635, 26693, 26806, 26812, 26813, 26898, 26923, 26947, 26959, 27143, 27144, 27442, 27443, 27542, 27927, 28207, 28309, 28602, 28684, 28926, 29822, 32208, 33432, 33939, 33968, 34671, 34791, 35235, 35880, 36209, 37523, 38294, 38585, 38777, 38795, 38814, 39497, 39543, 39846, 40128, 40129, 40166, 40418, 40566, 40567, 40568, 41089, 41325, 41683, 41750, 41800, 41817, 42235, 42266, 42455, 42665, 43270, 43793, 44412]\n"
     ]
    }
   ],
   "source": [
    "single_ipu = [i for i in range(len(x_data)) if x_data[i]['total_number_of_words_in_ipu'] == x_data[i]['word_number_in_ipu'] and x_data[i]['word_number_in_ipu'] == '1']\n",
    "single_turn = [i for i in range(len(x_data)) if x_data[i]['total_number_of_words_in_turn'] == x_data[i]['word_number_in_turn'] and x_data[i]['word_number_in_turn'] == '1']\n",
    "single_ipu_no_4 = [i for i in single_ipu if not y_data[i]]\n",
    "single_turn_no_4 = [i for i in single_turn if not y_data[i]]\n",
    "print(single_ipu_no_4)\n",
    "print(single_turn_no_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'1p': 256, '3p': 90, '2': 33, '2p': 21, '1': 12, '3': 66}) 478\n"
     ]
    }
   ],
   "source": [
    "single_ipu_no_4_breaks = defaultdict(int)\n",
    "for i in single_ipu_no_4:\n",
    "    single_ipu_no_4_breaks[labels[i]] += 1\n",
    "print(single_ipu_no_4_breaks, len(single_ipu_no_4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'1p': 83, '3p': 17, '2': 1, '2p': 2, '3': 6, '1': 1}) 110\n"
     ]
    }
   ],
   "source": [
    "single_turn_no_4_breaks = defaultdict(int)\n",
    "for i in single_turn_no_4:\n",
    "    single_turn_no_4_breaks[labels[i]] += 1\n",
    "print(single_turn_no_4_breaks, len(single_turn_no_4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
