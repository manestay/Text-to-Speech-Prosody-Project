import json
import logging
import pandas as pd
import string
from stanfordcorenlp import StanfordCoreNLP
from OrganizedBigTable import OrganizedBigTable, OrderType, _turnsToText

TABLE_NAME = 'games-data-20180223.csv'

START_SESSION = 12
END_SESSION = 13
FILLERS = set(['um','uh','uh-huh','hm','ah','mm','mmhm'])
ASSIMILATIONS = set(['gonna','gotta','lemme','gimme','wanna','dun','dunno','you`','you`re',
                     'they`', 'they`re'])
# note that 'cannot' is also an assimilation in CoreNLP, but we can ignore for this corpus
# dunno is lemmatized as du n no, so need 'dun' and 'dunno' to combine correctly
# 'you`re' and 'they`re' with backticks are generated by CoreNLP when they end a sentence, we need
# temporarily fixed by adding to this set

HYPHEN = '-'

def generate_column_data(ann, cleaned_indices):
    parsed = json.loads(ann)
    sentence = parsed['sentences'][0]

    syntactic_functions = []
    parse_trees = []
    word_list = []

    parse_tree = sentence['parse']
    parse_tree = ' '.join(parse_tree.split()) # collapse whitespace

    dep_list = sentence['enhancedPlusPlusDependencies']
    dep_list = clean_dep_list(dep_list)

    for i, d in enumerate(dep_list):
        # '''
        # example entry in list:
        # {'dep': 'nsubj', 'governorGloss': 'thoughlrt', 'dependent': 3, 'dependentGloss': 'I', 'governor': 4}
        # '''
        # print(d)
        word = d['dependentGloss']
        has_apostrophe = '\'' in word
        is_tis = word_list and word == 'is' and word_list[-1] == '\'t'
        is_end_of_assimilation = word_list and (word_list[-1] + word) in ASSIMILATIONS

        if word_list and has_apostrophe or is_end_of_assimilation or is_tis:
            # print(word_list[-1], word)
            syntactic_functions[-1] += '_{}'.format(d['dep'])
            word_list[-1] += word
        else:
            parse_trees.append(parse_tree)
            syntactic_functions.append(d['dep'])
            word_list.append(word)

    for i in cleaned_indices:
        # print(i)
        parse_trees.insert(i, '')
        syntactic_functions.insert(i, '')
        word_list.insert(i, 'FILLER')

    return parse_trees, syntactic_functions, word_list

def clean_dep_list(dep_list):
    '''
    Clean the dep_list by sorting and removing duplicates
    '''
    dep_list.sort(key=lambda x:x['dependent'])
    seen = set()
    cleaned_dep_list = []
    for d in dep_list:
        word_index = d['dependent']
        if word_index not in seen: # do not add words multiple times
            cleaned_dep_list.append(d)
        seen.add(word_index)
    return cleaned_dep_list

def get_sentence_text(sentence):
    return ' '.join([x['word'] for x in sentence['tokens']])

def clean_sentence(sentence):
    '''
    returns the cleaned sentence list, and a list of indices where a word was removed.
    '''
    indices = []
    word_list = []
    # print(sentence.strip().split(' '))
    for i, word in enumerate(sentence.strip().split(' ')):
        original_word = word
        word = word.translate(str.maketrans('','','-()?')) # fix words such as cha-(ir)
        word = word.replace('`', '\'')
        if word and word not in FILLERS and original_word[-1] != HYPHEN:
            word_list.append(word)
        else:
            indices.append(i)
    return word_list, indices

if __name__ == '__main__':
    props = {'annotators': 'tokenize,ssplit,pos,parse,depparse','outputFormat':'json'}
    with StanfordCoreNLP('/mnt/c/Users/coolw/Dropbox/Programming/corenlp/', memory='3g', quiet=True) as client:
        for session_number in range(START_SESSION, END_SESSION):
            print('SESSION {}'.format(session_number))
            parse_trees = []
            syntactic_functions = []
            word_list = []
            bigtable = OrganizedBigTable(session_number, OrderType.GIVEN, TABLE_NAME, clean=False)
            bigtable.limitDataFrameToSession()
            text = _turnsToText(bigtable.all_turns)

            sentences = [x for x in text.split('.') if x]
            for sentence in sentences:
                sentence_list, cleaned_indices = clean_sentence(sentence)
                if not sentence_list: # sentence was only fillers or stutters
                    for _ in range(len(cleaned_indices)):
                        parse_trees.append('')
                        syntactic_functions.append('')
                        word_list.append('FILLER SENTENCE')
                else:
                    # sentence_list.append('.')
                    cleaned_sentence = ' '.join(sentence_list)
                    ann = client.annotate(cleaned_sentence, properties=props)
                    pts, sfs, wlnew = generate_column_data(ann, cleaned_indices)
                    parse_trees += pts
                    syntactic_functions += sfs
                    word_list += wlnew
                    print(sentence, '||||||', cleaned_sentence)
            bigtable.addColumnToDataFrameInPlace(word_list, 'word_check')
            bigtable.addColumnToDataFrameInPlace(parse_trees, 'parse_tree')
            bigtable.addColumnToDataFrameInPlace(syntactic_functions, 'syntactic_function')
            bigtable.saveToCSV()
